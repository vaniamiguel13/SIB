{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Passando agora para a sub-área do Machine Learning, o Deep Learning permite tratar dados mais extensos com maior facilidade e precisão (em muitos casos).\n",
    "\n",
    "Visto que o código desenvolvido anteriormente é bastante extenso e demorado, vamos primeiro definir os blocos de código essenciais de correr antes de efetuar a análise por Deep Learning (de forma a poder começar a análise *de novo* a partir daqui)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multilayer Perceptron"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "X_train_sc = pd.read_csv(\"Files/x_train_sc.csv\", index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "y_train = pd.read_csv(\"Files/y_train.csv\", index_col=0)\n",
    "y_train = pd.Series(y_train[\"tm\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tendo definido as 200 features com a maior importância (no ficheiro \"SL.ipynb\"), e que permitiram efetuar previsões com precisão relativamente elevada, vamos utilizar as mesmas (geradas através do método da Informação Mútua) para prosseguir com a análise utilizando 'Deep Learning'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# get k best scores between features and label -> pearson, spearman, f_regression and multi_info_regression\n",
    "def get_k_best_corrs(k, scores):\n",
    "    idxs = np.argsort(scores)[-k:]\n",
    "    feats = X_train_sc.columns[idxs]\n",
    "    scores = np.sort(scores)[-k:]\n",
    "    return {f: c for f, c in zip(feats, scores)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "mutual_info = [float(elem.strip()) for elem in open(\"Files/mutual_info.txt\").readlines()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "feature_names = get_k_best_corrs(200, mutual_info).keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "x_train_mi = X_train_sc.loc[:, feature_names]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                 AA            DG            LR  _SecondaryStrC3  \\\ncount  28403.000000  28403.000000  28403.000000     28403.000000   \nmean       0.077555      0.097300      0.115780         0.284679   \nstd        0.073396      0.102072      0.116764         0.062164   \nmin        0.000000      0.000000      0.000000         0.000000   \n25%        0.023330      0.000000      0.032000         0.246790   \n50%        0.062566      0.078431      0.090000         0.278174   \n75%        0.113468      0.151961      0.160000         0.315264   \nmax        1.000000      1.000000      1.000000         1.000000   \n\n                 AI  _SolventAccessibilityT13            GS            VA  \\\ncount  28403.000000              28403.000000  28403.000000  28403.000000   \nmean       0.096641                  0.232627      0.018175      0.107986   \nstd        0.096481                  0.056298      0.019162      0.096550   \nmin        0.000000                  0.000000      0.000000      0.000000   \n25%        0.000000                  0.196328      0.000000      0.038000   \n50%        0.076923                  0.231638      0.015600      0.092000   \n75%        0.140659                  0.269774      0.026400      0.150000   \nmax        1.000000                  1.000000      1.000000      1.000000   \n\n                 ST            GL  ...  _SecondaryStrD3001  MolecularWeight  \\\ncount  28403.000000  28403.000000  ...        28403.000000     28403.000000   \nmean       0.059385      0.112944  ...            0.017738         0.013734   \nstd        0.062607      0.102629  ...            0.021898         0.020303   \nmin        0.000000      0.000000  ...            0.000000         0.000000   \n25%        0.000000      0.036275  ...            0.005061         0.005909   \n50%        0.051240      0.098039  ...            0.010732         0.010132   \n75%        0.095868      0.170588  ...            0.021564         0.016075   \nmax        1.000000      1.000000  ...            1.000000         1.000000   \n\n       _PolarizabilityD1001  _PolarityD2001  _HydrophobicityD2001  \\\ncount          28403.000000    28403.000000          28403.000000   \nmean               0.021564        0.038799              0.040168   \nstd                0.026867        0.055728              0.055127   \nmin                0.000000        0.000000              0.000000   \n25%                0.006335        0.010179              0.011252   \n50%                0.012451        0.020096              0.021828   \n75%                0.027561        0.044338              0.046582   \nmax                1.000000        1.000000              1.000000   \n\n       _NormalizedVDWVD1001             S             N             I  \\\ncount          28403.000000  28403.000000  28403.000000  28403.000000   \nmean               0.036351      0.187294      0.134017      0.274215   \nstd                0.042654      0.070865      0.057128      0.108220   \nmin                0.000000      0.000000      0.000000      0.000000   \n25%                0.011002      0.135745      0.096186      0.206300   \n50%                0.021478      0.181189      0.129072      0.268700   \n75%                0.046157      0.230302      0.165098      0.335350   \nmax                1.000000      1.000000      1.000000      1.000000   \n\n                  Q  \ncount  28403.000000  \nmean       0.167674  \nstd        0.077459  \nmin        0.000000  \n25%        0.121437  \n50%        0.157973  \n75%        0.206071  \nmax        1.000000  \n\n[8 rows x 200 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AA</th>\n      <th>DG</th>\n      <th>LR</th>\n      <th>_SecondaryStrC3</th>\n      <th>AI</th>\n      <th>_SolventAccessibilityT13</th>\n      <th>GS</th>\n      <th>VA</th>\n      <th>ST</th>\n      <th>GL</th>\n      <th>...</th>\n      <th>_SecondaryStrD3001</th>\n      <th>MolecularWeight</th>\n      <th>_PolarizabilityD1001</th>\n      <th>_PolarityD2001</th>\n      <th>_HydrophobicityD2001</th>\n      <th>_NormalizedVDWVD1001</th>\n      <th>S</th>\n      <th>N</th>\n      <th>I</th>\n      <th>Q</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>...</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n      <td>28403.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.077555</td>\n      <td>0.097300</td>\n      <td>0.115780</td>\n      <td>0.284679</td>\n      <td>0.096641</td>\n      <td>0.232627</td>\n      <td>0.018175</td>\n      <td>0.107986</td>\n      <td>0.059385</td>\n      <td>0.112944</td>\n      <td>...</td>\n      <td>0.017738</td>\n      <td>0.013734</td>\n      <td>0.021564</td>\n      <td>0.038799</td>\n      <td>0.040168</td>\n      <td>0.036351</td>\n      <td>0.187294</td>\n      <td>0.134017</td>\n      <td>0.274215</td>\n      <td>0.167674</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.073396</td>\n      <td>0.102072</td>\n      <td>0.116764</td>\n      <td>0.062164</td>\n      <td>0.096481</td>\n      <td>0.056298</td>\n      <td>0.019162</td>\n      <td>0.096550</td>\n      <td>0.062607</td>\n      <td>0.102629</td>\n      <td>...</td>\n      <td>0.021898</td>\n      <td>0.020303</td>\n      <td>0.026867</td>\n      <td>0.055728</td>\n      <td>0.055127</td>\n      <td>0.042654</td>\n      <td>0.070865</td>\n      <td>0.057128</td>\n      <td>0.108220</td>\n      <td>0.077459</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.023330</td>\n      <td>0.000000</td>\n      <td>0.032000</td>\n      <td>0.246790</td>\n      <td>0.000000</td>\n      <td>0.196328</td>\n      <td>0.000000</td>\n      <td>0.038000</td>\n      <td>0.000000</td>\n      <td>0.036275</td>\n      <td>...</td>\n      <td>0.005061</td>\n      <td>0.005909</td>\n      <td>0.006335</td>\n      <td>0.010179</td>\n      <td>0.011252</td>\n      <td>0.011002</td>\n      <td>0.135745</td>\n      <td>0.096186</td>\n      <td>0.206300</td>\n      <td>0.121437</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.062566</td>\n      <td>0.078431</td>\n      <td>0.090000</td>\n      <td>0.278174</td>\n      <td>0.076923</td>\n      <td>0.231638</td>\n      <td>0.015600</td>\n      <td>0.092000</td>\n      <td>0.051240</td>\n      <td>0.098039</td>\n      <td>...</td>\n      <td>0.010732</td>\n      <td>0.010132</td>\n      <td>0.012451</td>\n      <td>0.020096</td>\n      <td>0.021828</td>\n      <td>0.021478</td>\n      <td>0.181189</td>\n      <td>0.129072</td>\n      <td>0.268700</td>\n      <td>0.157973</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.113468</td>\n      <td>0.151961</td>\n      <td>0.160000</td>\n      <td>0.315264</td>\n      <td>0.140659</td>\n      <td>0.269774</td>\n      <td>0.026400</td>\n      <td>0.150000</td>\n      <td>0.095868</td>\n      <td>0.170588</td>\n      <td>...</td>\n      <td>0.021564</td>\n      <td>0.016075</td>\n      <td>0.027561</td>\n      <td>0.044338</td>\n      <td>0.046582</td>\n      <td>0.046157</td>\n      <td>0.230302</td>\n      <td>0.165098</td>\n      <td>0.335350</td>\n      <td>0.206071</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 200 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_mi.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "0        48.4\n1        48.4\n2        49.0\n3        55.6\n4        48.4\n         ... \n28691    51.8\n28692    37.2\n28693    64.6\n28694    50.7\n28695    37.6\nName: tm, Length: 28403, dtype: float64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos criar uma rede neuronal constituída por camadas densas (o tipo de rede mais simples e utilizada) para poder efetuar previsões relativamente aos valores de termostabilidade. Este tipo de redes são também conhecidos como \"Multilayer Perceptrons\" (ou MLP)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "#!pip install scikeras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "import itertools"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visto que este tipo de redes (assim como outros) apresenta vários hiperparâmetros que podem ser otimizados para obter um modelo que permite uma previsão mais precisa, vamos criar uma função (chamada *create_model()*) que permite a adição de um número variável de camadas, cada uma com valores variáveis de nodos (e outros parâmetros que vão ser discutidos de seguida). A cada camada densa é adicionado também uma regularização dropout (para evitar sobreajustamentos), e por fim, a rede é compilada com a selecção de um dos otimizadores mais utilizados (selecionado através da função auxiliar *_choose_optimizer()*).\n",
    "\n",
    "Os seguintes blocos de código foram inspirados no código apresentado no seguinte site: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "#GRID SEARCH FUNC\n",
    "\n",
    "def _choose_optimizer(optimizer, learning_rate, momentum):\n",
    "    if optimizer == \"sgd\":\n",
    "        return SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "    elif optimizer == \"rmsprop\":\n",
    "        return RMSprop(learning_rate=learning_rate, momentum=momentum)\n",
    "    elif optimizer == \"adagrad\":\n",
    "        return Adagrad(learning_rate=learning_rate)\n",
    "    elif optimizer == \"adadelta\":\n",
    "        return Adadelta(learning_rate=learning_rate)\n",
    "    elif optimizer == \"adam\":\n",
    "        return Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == \"adamax\":\n",
    "        return Adamax(learning_rate=learning_rate)\n",
    "    elif optimizer == \"nadam\":\n",
    "        return Nadam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized optimizer\")\n",
    "\n",
    "\n",
    "def create_model(first_input, layers, neurons, dropout_rate, weight_constraint, learning_rate, momentum, activation, init_mode='uniform', optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    for ix in range(layers):\n",
    "        if ix == 0:\n",
    "            model.add(Dense(neurons[ix],\n",
    "                            activation=activation,\n",
    "                            input_shape=first_input,\n",
    "                            kernel_initializer=init_mode,\n",
    "                            kernel_constraint=MaxNorm(weight_constraint)))\n",
    "        elif ix != layers-1:\n",
    "            model.add(Dense(neurons[ix],\n",
    "                            activation=activation,\n",
    "                            kernel_initializer=init_mode,\n",
    "                            kernel_constraint=MaxNorm(weight_constraint)))\n",
    "        else:\n",
    "            model.add(Dense(1,\n",
    "                            activation=activation,\n",
    "                            kernel_initializer=init_mode))\n",
    "\n",
    "        if ix != layers-1:\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "    opt = _choose_optimizer(optimizer.lower(), learning_rate, momentum)\n",
    "\n",
    "    model.compile(loss=\"mse\", optimizer=opt, metrics=[\"mse\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "#Parameter values to optimize:\n",
    "neurons = [grid + (1,) for grid in itertools.product(*[[20,50,100]]*3)]\n",
    "dropout_rate = np.linspace(0.0, 0.5, 6)\n",
    "weight_constraint = np.linspace(0.5, 5, 10) #Max norm value each weight parameter can be\n",
    "learning_rate = np.linspace(0.005, 0.5, 100)\n",
    "momentum = np.linspace(0.0, 0.9, 4) #9\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "\n",
    "#For computational purposes, all layers will have the same  weight_constraint and dropout_rate values\n",
    "\n",
    "param_dist = dict(model__neurons=neurons,\n",
    "                  model__dropout_rate=dropout_rate,\n",
    "                  model__weight_constraint=weight_constraint,\n",
    "                  model__learning_rate=learning_rate,\n",
    "                  model__momentum=momentum,\n",
    "                  model__optimizer=optimizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving best parameters for a 4 layered fully-connected network (n_iter=50, cv=5):\n",
      "|\n",
      "Best score: 0.370032506271634\n",
      "Best parameters: {'model__weight_constraint': 2.5, 'model__optimizer': 'Adadelta', 'model__neurons': (100, 100, 50, 1), 'model__momentum': 0.9, 'model__learning_rate': 0.465, 'model__dropout_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "param_dist[\"model__neurons\"] = [grid + (1,) for grid in itertools.product(*[[20,50,100]]*3)]\n",
    "\n",
    "print(f\"Retrieving best parameters for a 4 layered fully-connected network (n_iter=50, cv=5):\\n|\")\n",
    "model = KerasRegressor(model=create_model, epochs=100, batch_size=10, verbose=0,\n",
    "                   first_input=[x_train_mi.shape[1]],\n",
    "                   layers=4,\n",
    "                   activation=\"relu\")\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=50, n_jobs=-1, cv=5)\n",
    "grid.fit(x_train_mi, y_train)\n",
    "print(f\"Best score: {grid.best_score_}\")\n",
    "print(f\"Best parameters: {grid.best_params_}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving best parameters for a 6 layered fully-connected network (n_iter=50, cv=5):\n",
      "|\n",
      "(20, 50, 50, 20, 100, 1)\n",
      "Best score: -0.010840052742769002\n",
      "Best parameters: {'model__weight_constraint': 0.5, 'model__optimizer': 'Adagrad', 'model__neurons': (20, 50, 50, 20, 100, 1), 'model__momentum': 0.3, 'model__learning_rate': 0.255, 'model__dropout_rate': 0.5}\n"
     ]
    }
   ],
   "source": [
    "param_dist[\"model__neurons\"] = [grid + (1,) for grid in itertools.product(*[[20,50,100]]*5)]\n",
    "\n",
    "print(f\"Retrieving best parameters for a 6 layered fully-connected network (n_iter=50, cv=5):\\n|\")\n",
    "model = KerasRegressor(model=create_model, epochs=100, batch_size=10, verbose=0,\n",
    "                   first_input=[x_train_mi.shape[1]],\n",
    "                   layers=6,\n",
    "                   activation=\"relu\")\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=1, n_jobs=-1, cv=5)\n",
    "grid.fit(x_train_mi, y_train)\n",
    "print(f\"Best score: {grid.best_score_}\")\n",
    "print(f\"Best parameters: {grid.best_params_}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving best parameters for a 8 layered fully-connected network (n_iter=50, cv=5):\n",
      "|\n",
      "Best score: -0.009267656321677187\n",
      "Best parameters: {'model__weight_constraint': 3.5, 'model__optimizer': 'Adamax', 'model__neurons': (20, 50, 100, 20, 100, 20, 20, 1), 'model__momentum': 0.0, 'model__learning_rate': 0.17, 'model__dropout_rate': 0.5}\n"
     ]
    }
   ],
   "source": [
    "param_dist[\"model__neurons\"] = [grid + (1,) for grid in itertools.product(*[[20,50,100]]*7)]\n",
    "\n",
    "print(f\"Retrieving best parameters for a 8 layered fully-connected network (n_iter=50, cv=5):\\n|\")\n",
    "model = KerasRegressor(model=create_model, epochs=100, batch_size=10, verbose=0,\n",
    "                   first_input=[x_train_mi.shape[1]],\n",
    "                   layers=8,\n",
    "                   activation=\"relu\")\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=1, n_jobs=-1, cv=5)\n",
    "grid.fit(x_train_mi, y_train)\n",
    "print(f\"Best score: {grid.best_score_}\")\n",
    "print(f\"Best parameters: {grid.best_params_}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como se pode observar pelos resultados, o modelo com o melhor desempenho (valor mínimo de 'mse') foi o que apresentava uma rede contendo 8 camadas densas (usando os valores de hiperparâmetros indicados no output), obtendo um score de **0.0097**.\n",
    "\n",
    "Testando um maior conjunto de valores de hiperparâmetros e número de camadas densas, era possível encontrar um modelo ainda mais preciso. Contudo, este processo tem um peso computacional significativo, o que dificulta a procura do melhor conjunto de hiperparâmetros."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

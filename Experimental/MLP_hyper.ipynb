{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "j8pd3zJMWmXJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHxzw7fEHjpX"
   },
   "source": [
    "## Randomized hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "b4oDLI3kXcuk"
   },
   "outputs": [],
   "source": [
    "def setup_model(num_layers, k_constr, dropout_rate, input_size, output_size):\n",
    "    # initialize sequential dnn model\n",
    "    model = tf.keras.Sequential()\n",
    "    # initialize lower and upper bounds to randomly pick the number of neurons at each layer\n",
    "    lb, ub = output_size+1, input_size\n",
    "    # main loop -> setup dnn model (generate dnn topology)\n",
    "    topology = []\n",
    "    for i in range(num_layers):\n",
    "        ub = random.randint(lb, ub)\n",
    "        topology.append(ub)\n",
    "        model.add(tf.keras.layers.Dense(ub,\n",
    "                                        activation=\"relu\",\n",
    "                                        kernel_constraint=tf.keras.constraints.max_norm(k_constr),\n",
    "                                        input_dim=input_size if i == 0 else None))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    # add last layer to dnn (dim output = 1 with linear activation)\n",
    "    model.add(tf.keras.layers.Dense(output_size,\n",
    "                                    kernel_constraint=tf.keras.constraints.max_norm(k_constr),\n",
    "                                    activation=\"linear\"))\n",
    "    return topology, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "8QPesLVoZp72"
   },
   "outputs": [],
   "source": [
    "def cv_dnn(model, optimizer, X_train, y_train, epochs=400, batch_size=64, cv=5, val_size=0.3):\n",
    "    # cross-validate dnn (repeated random subsampling)\n",
    "    num_val = int(X_train.shape[0] * val_size)\n",
    "    val_losses = []\n",
    "    for _ in range(cv):\n",
    "        shuffle = np.random.permutation(X_train.shape[0])\n",
    "        x_train_shuf, y_train_shuf = X_train.iloc[shuffle, :], y_train.iloc[shuffle, :]\n",
    "        x_trn, y_trn = x_train_shuf.iloc[num_val:, :], y_train_shuf.iloc[num_val:, :]\n",
    "        x_val, y_val = x_train_shuf.iloc[:num_val, :], y_train_shuf.iloc[:num_val, :]\n",
    "        # compile and fit dnn model                                                                     \n",
    "        model.compile(optimizer=optimizer, loss=\"mse\") # metrics=[\"mse\"] ideally should be r2_score\n",
    "        model.fit(x_trn, y_trn, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        val_loss = model.evaluate(x_val, y_val, verbose=0)\n",
    "        val_losses.append(val_loss)\n",
    "    return np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "oW2nsQrXyJr0"
   },
   "outputs": [],
   "source": [
    "k_opt = tf.keras.optimizers\n",
    "OPTIMIZERS = {\"adam\": k_opt.Adam(),\n",
    "              \"rmsprop\": k_opt.RMSprop(),\n",
    "              \"sgd\": k_opt.SGD(),\n",
    "              \"sgd_momentum\": k_opt.SGD()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "5uDWEwmlbfJx"
   },
   "outputs": [],
   "source": [
    "def dnn_randomized_opt(param_grid, X_train, y_train, n_iter=10, verbose=True):\n",
    "    # get input and output sizes\n",
    "    input_size, output_size = X_train.shape[1], y_train.shape[1]\n",
    "    # set up search space for hyper-parameter optimization\n",
    "    num_layers = param_grid[\"num_layers\"] if \"num_layers\" in param_grid else [4]\n",
    "    k_constrs = param_grid[\"kernel_constraint\"] if \"kernel_constraint\" in param_grid else [np.inf]\n",
    "    dropouts = param_grid[\"dropout\"] if \"dropout\" in param_grid else [0.0]\n",
    "    lrs = param_grid[\"learning_rate\"] if \"learning_rate\" in param_grid else [0.001]\n",
    "    algs = param_grid[\"algorithm\"] if \"algorithm\" in param_grid else [\"adam\"]\n",
    "    mms = param_grid[\"momentum\"] if \"momentum\" in param_grid else [0.9]\n",
    "    # initialize best_loss\n",
    "    best_loss = np.inf\n",
    "    # main loop -> optimize dnn hyper-parameters\n",
    "    for i in range(n_iter):\n",
    "        # randomly choose the number of dense layers, kernel_constraint and dropout rate -> to setup dnn model\n",
    "        init_ = random.choice(num_layers), random.choice(k_constrs), random.choice(dropouts)\n",
    "        n_layers, k_constr, dropout_rate = init_\n",
    "        topo, dnn_init = setup_model(*init_, input_size, output_size)\n",
    "        # randomly pick learning rate, optimizer and momentum (if applicable), and cross-validate dnn model\n",
    "        alg, lr = random.choice(algs), random.choice(lrs)\n",
    "        optimizer = OPTIMIZERS[alg]\n",
    "        setattr(optimizer, \"lr\", lr)\n",
    "        if alg == \"sgd_momentum\":\n",
    "            mm = random.choice(mms)\n",
    "            setattr(optimizer, \"momentum\", mm)\n",
    "        val_loss = cv_dnn(dnn_init, optimizer, X_train, y_train)\n",
    "        # print iter information if verbose\n",
    "        if verbose:\n",
    "            # before printing, assign mm (momentum) if the optimizer is not \"sgd_momentum\"\n",
    "            if alg != \"sgd_momentum\": mm = \"NA\"\n",
    "            print(f\"ITERATION {i+1}\\nTopology: {topo}\\nKernel constraint: {k_constr}\\nDropout: {dropout_rate}\\n\"\n",
    "                  f\"Algorithm: {alg}\\nLearning rate: {lr}\\nMomentum: {mm}\\nValidation loss: {val_loss}\\n\")\n",
    "        # update best_acc and best_config if the attained result is the best so far\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_config = (topo, k_constr, dropout_rate, alg, lr, mm)\n",
    "    # \"cast\" best_config tuple to dictionary\n",
    "    topo, k_constr, dropout_rate, alg, lr, mm = best_config\n",
    "    best_config = {\"topology\": topo, \"kernel_constraint\": k_constr, \"dropout\": dropout_rate,\n",
    "                   \"algorithm\": alg, \"learning_rate\": lr, \"momentum\": mm}\n",
    "    return best_loss, best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrsBDZbmOjHU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5725a1ab-4187-4b44-9100-e4dd166f0d8e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_sc = pd.read_csv(\"X_train_sc.csv\", index_col=0)\n",
    "y_train = pd.read_csv(\"y_train.csv\", index_col=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get k best scores between features and label -> pearson, spearman, f_regression and multi_info_regression\n",
    "def get_k_best_corrs(k, scores):\n",
    "    idxs = np.argsort(scores)[-k:]\n",
    "    feats = X_train_sc.columns[idxs]\n",
    "    scores = np.sort(scores)[-k:]\n",
    "    return {f: c for f, c in zip(feats, scores)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pearson_corrs = [float(elem.strip()) for elem in open(\"pearson.txt\").readlines()]\n",
    "spearman_corrs = [float(elem.strip()) for elem in open(\"spearman.txt\").readlines()]\n",
    "f_values = [float(elem.strip()) for elem in open(\"f_values.txt\").readlines()]\n",
    "mutual_info = [float(elem.strip()) for elem in open(\"mutual_info.txt\").readlines()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "6f33b782-54c4-40c5-8bcb-1551754bb2a6",
   "metadata": {},
   "source": [
    "Por fim, aplicámos algoritmos de aprendizagem supervisionada de modo a conseguirmos fazer previsões acerca da variável de output (<b>tm</b>) a partir de sequências de aminoácidos cujo valor de termostabilidade é desconhecido. Para isso, utilizámos sete algoritmos distintos implementados na biblioteca <b>sklearn</b>.\n",
    "\n",
    "- <b>LinearRegression</b> (LR)\n",
    "- <b>KNeighborsRegressor</b> (KNR)\n",
    "- <b>RandomForestRegressor</b> (RFR)\n",
    "- <b>SVR</b> -> Support Vector Regressor\n",
    "- <b>MLPRegressor</b> (MLPR) -> Multi Layer Perceptron Regressor\n",
    "- <b>AdaBoostRegressor</b> (ADA)\n",
    "- <b>HistGradientBoostingRegressor</b> (HGBR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc515b9c-8d8f-4772-9153-b700e8454baa",
   "metadata": {},
   "source": [
    "### Get best combination dataset-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2ff20ec-333d-426f-a9fb-7531c1658800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor as ADA\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor as HGBR\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNR\n",
    "from sklearn.neural_network import MLPRegressor as MLPR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ee745-d87f-495d-a580-4b902a5e4c2f",
   "metadata": {},
   "source": [
    "Começámos por determinar, a partir de cada um dos métodos de seleção de features referidos anteriormente (<b>Pearson</b>, <b>Spearman</b>, <b>ANOVA f-values</b> e <b>informação mútua</b>), qual a melhor combinação <b>modelo / número de features selecionadas</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62077a11-790f-431f-8422-b78393b31403",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUMS = [200, 400, 600, 800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd667d78-a525-4f69-9a23-adf08ba73cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_corr_models(model, method, corrs, cv=5, num_feats=DEFAULT_NUMS):\n",
    "    print(f\"Getting results for {model.__class__.__name__} using {method} to select features...\\n\")\n",
    "    for k in num_feats:\n",
    "        # get best corrs and cross-validate\n",
    "        best_scores = get_k_best_corrs(k, corrs) # Get best features\n",
    "        result = cross_validate(estimator=model,\n",
    "                                X=X_train_sc.loc[:, best_scores.keys()],\n",
    "                                y=y_train,\n",
    "                                cv=KFold(n_splits=cv, shuffle=True),\n",
    "                                return_train_score=True)\n",
    "        # print results\n",
    "        mean_train = np.sum(result[\"train_score\"]) / cv\n",
    "        mean_test = np.sum(result[\"test_score\"]) / cv\n",
    "        print(f\"Results for {k} best features\")\n",
    "        print(f\"Train scores: {result['train_score']} -> {mean_train = :.4f}\")\n",
    "        print(f\"Test scores: {result['test_score']} -> {mean_test = :.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab4536-e6b1-422f-b881-8e9f5b8b87e9",
   "metadata": {},
   "source": [
    "<b>Pearson</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "edf92216",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for LinearRegression using Pearson correlation to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.31830325 0.31392415 0.31633945 0.31182916 0.3066358 ] -> mean_train = 0.3134\n",
      "Test scores: [0.27919092 0.29974493 0.29115746 0.30118958 0.33140716] -> mean_test = 0.3005\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.39371228 0.38799135 0.39360352 0.39383614 0.39987173] -> mean_train = 0.3938\n",
      "Test scores: [0.38300769 0.35232374 0.37767527 0.37743098 0.35830092] -> mean_test = 0.3697\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.42371335 0.4146264  0.42294396 0.42536268 0.42076352] -> mean_train = 0.4215\n",
      "Test scores: [0.37103912 0.37512818 0.38437575 0.3865769  0.40484428] -> mean_test = 0.3844\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.43373677 0.43318539 0.43969612 0.43861505 0.43600294] -> mean_train = 0.4362\n",
      "Test scores: [0.41054996 0.3655078  0.38919964 0.39444717 0.36578661] -> mean_test = 0.3851\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for RandomForestRegressor using Pearson correlation to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.86762202 0.86455812 0.86756847 0.8654831  0.86635326] -> mean_train = 0.8663\n",
      "Test scores: [0.46893692 0.47474253 0.47183767 0.47635708 0.46603657] -> mean_test = 0.4716\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.86865287 0.8708922  0.86909876 0.86407208 0.87073908] -> mean_train = 0.8687\n",
      "Test scores: [0.48321788 0.4651082  0.48409388 0.51644364 0.45168042] -> mean_test = 0.4801\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.86692848 0.86931937 0.86943738 0.86686052 0.87293123] -> mean_train = 0.8691\n",
      "Test scores: [0.4851075  0.48528567 0.49189294 0.52378137 0.46479614] -> mean_test = 0.4902\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.86701671 0.86933204 0.87158342 0.86787022 0.87067643] -> mean_train = 0.8693\n",
      "Test scores: [0.50926646 0.47984863 0.4709283  0.48995825 0.4742135 ] -> mean_test = 0.4848\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for SVR using Pearson correlation to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.27768877 0.28164607 0.27543778 0.28223729 0.27169098] -> mean_train = 0.2777\n",
      "Test scores: [0.27372642 0.26373777 0.28260736 0.2527845  0.28642668] -> mean_test = 0.2719\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.26833298 0.26359706 0.26154189 0.26092158 0.27139692] -> mean_train = 0.2652\n",
      "Test scores: [0.25027263 0.26140355 0.27630936 0.2633784  0.24690274] -> mean_test = 0.2597\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.25604641 0.2557586  0.2542764  0.26941398 0.26639608] -> mean_train = 0.2604\n",
      "Test scores: [0.26574554 0.25771847 0.27268954 0.23138731 0.24560992] -> mean_test = 0.2546\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.25269249 0.25810787 0.25484969 0.25800937 0.26009149] -> mean_train = 0.2568\n",
      "Test scores: [0.25179683 0.25459869 0.26067716 0.24756907 0.23981551] -> mean_test = 0.2509\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for MLPRegressor using Pearson correlation to select features...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 200 best features\n",
      "Train scores: [0.52105186 0.51666136 0.51690926 0.52675541 0.52715188] -> mean_train = 0.5217\n",
      "Test scores: [0.43688842 0.45549741 0.42600597 0.45694164 0.44466639] -> mean_test = 0.4440\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 400 best features\n",
      "Train scores: [0.46204741 0.49887642 0.4694671  0.45133553 0.50712451] -> mean_train = 0.4778\n",
      "Test scores: [0.42765406 0.46299803 0.43444477 0.43355555 0.45313489] -> mean_test = 0.4424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 600 best features\n",
      "Train scores: [0.50828932 0.48730197 0.53997773 0.52672009 0.51295359] -> mean_train = 0.5150\n",
      "Test scores: [0.48259759 0.42385795 0.48960782 0.47330764 0.47066749] -> mean_test = 0.4680\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 800 best features\n",
      "Train scores: [0.55768337 0.53201379 0.57710713 0.54583701 0.53376656] -> mean_train = 0.5493\n",
      "Test scores: [0.50072421 0.44872076 0.47927216 0.48096196 0.48445523] -> mean_test = 0.4788\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for AdaBoostRegressor using Pearson correlation to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.30878064 0.31747994 0.2821283  0.33309379 0.30994882] -> mean_train = 0.3103\n",
      "Test scores: [0.31653165 0.28572524 0.27650359 0.31991506 0.28990877] -> mean_test = 0.2977\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.31737068 0.33604038 0.30093692 0.30342464 0.29648123] -> mean_train = 0.3109\n",
      "Test scores: [0.3274757  0.33384534 0.29182668 0.27977979 0.28082602] -> mean_test = 0.3028\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.31718018 0.28876691 0.30003358 0.29836457 0.32519596] -> mean_train = 0.3059\n",
      "Test scores: [0.29306934 0.26952092 0.28428521 0.30487058 0.31365367] -> mean_test = 0.2931\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.28814453 0.30092214 0.32320997 0.28723039 0.34017101] -> mean_train = 0.3079\n",
      "Test scores: [0.26270269 0.30130176 0.30585807 0.27255919 0.32357272] -> mean_test = 0.2932\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for HistGradientBoostingRegressor using Pearson correlation to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.61760073 0.61823471 0.62163874 0.62513078 0.62371354] -> mean_train = 0.6213\n",
      "Test scores: [0.52983723 0.53351532 0.51879814 0.49154409 0.50710966] -> mean_test = 0.5162\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.65205709 0.64711572 0.64902128 0.65037379 0.65255898] -> mean_train = 0.6502\n",
      "Test scores: [0.52083674 0.54288122 0.55053516 0.52329081 0.5269    ] -> mean_test = 0.5329\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.6641068  0.66170629 0.66909358 0.64717522 0.66185312] -> mean_train = 0.6608\n",
      "Test scores: [0.5458077  0.55594568 0.51133182 0.54347462 0.54799937] -> mean_test = 0.5409\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.66427657 0.66246132 0.65999607 0.66341141 0.67036066] -> mean_train = 0.6641\n",
      "Test scores: [0.54249578 0.54910128 0.52700264 0.54868719 0.52947596] -> mean_test = 0.5394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, Model in enumerate([LR, RFR, SVR, MLPR, ADA, HGBR]):\n",
    "    test_corr_models(model=Model(), method=\"Pearson correlation\", corrs=pearson_corrs)\n",
    "    if i < 5:\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32293ce6-597d-40f9-a8b4-d9bcbf06671f",
   "metadata": {},
   "source": [
    "<b>Pearson</b> (further testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25586e2a-f437-4ee5-be32-ecd60fa5a41c",
   "metadata": {},
   "source": [
    "Novos testes para um novo modelo (<b>KNeighborsRegressor</b>) e para uma gama de features distinta da anterior (para os modelos <b>SVR</b>, <b>MLPRegressor</b> e <b>HistGradientBoostingRegressor</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "042156fd-b2f6-4b19-b465-da540458166f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for KNeighborsRegressor using Pearson correlation to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.58503702 0.58997743 0.59379933 0.58816645 0.58779664] -> mean_train = 0.5890\n",
      "Test scores: [0.3928789  0.36819901 0.36770372 0.38222477 0.39573199] -> mean_test = 0.3813\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.61136161 0.60893551 0.60805813 0.61107387 0.60937698] -> mean_train = 0.6098\n",
      "Test scores: [0.41522946 0.427479   0.40978524 0.41295209 0.41445057] -> mean_test = 0.4160\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.61533186 0.61624814 0.61941333 0.6153637  0.62533147] -> mean_train = 0.6183\n",
      "Test scores: [0.43304784 0.4479805  0.43914913 0.43093723 0.39303128] -> mean_test = 0.4288\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.6232438  0.6171689  0.62247578 0.61839789 0.62404356] -> mean_train = 0.6211\n",
      "Test scores: [0.41433254 0.43647186 0.4307311  0.4435146  0.42248312] -> mean_test = 0.4295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_corr_models(model=KNR(), method=\"Pearson correlation\", corrs=pearson_corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e70e8d9-f390-421a-af19-b18bc6c407de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for SVR using Pearson correlation to select features...\n",
      "\n",
      "Results for 2000 best features\n",
      "Train scores: [0.25385633 0.24365601 0.24720895 0.2442057  0.25017659] -> mean_train = 0.2478\n",
      "Test scores: [0.22252446 0.23985421 0.24987935 0.2456937  0.24385969] -> mean_test = 0.2404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_corr_models(model=SVR(), method=\"Pearson correlation\", corrs=pearson_corrs, num_feats=[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "37b20309-bb1a-4a43-9a3d-4fff5ec4a8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for MLPRegressor using Pearson correlation to select features...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 1000 best features\n",
      "Train scores: [0.56490654 0.55768887 0.55608917 0.55828499 0.52461717] -> mean_train = 0.5523\n",
      "Test scores: [0.46331716 0.46617747 0.48015719 0.49865836 0.47098379] -> mean_test = 0.4759\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 1200 best features\n",
      "Train scores: [0.60046975 0.62332129 0.58112659 0.60393118 0.58597108] -> mean_train = 0.5990\n",
      "Test scores: [0.46578919 0.49174316 0.497005   0.47763045 0.49677766] -> mean_test = 0.4858\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antonioduarte/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_corr_models(model=MLPR(), method=\"Pearson correlation\", corrs=pearson_corrs, num_feats=[1000, 1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d32bc681-3c20-4137-9bba-4b133540f837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for HistGradientBoostingRegressor using Pearson correlation to select features...\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.66663238 0.66909042 0.66829338 0.64459554 0.64295293] -> mean_train = 0.6583\n",
      "Test scores: [0.5530253  0.53940209 0.53295276 0.53978152 0.52986614] -> mean_test = 0.5390\n",
      "\n",
      "Results for 900 best features\n",
      "Train scores: [0.65743789 0.66979245 0.66498664 0.6695574  0.66648605] -> mean_train = 0.6657\n",
      "Test scores: [0.52125799 0.53635422 0.54399451 0.55205156 0.55057303] -> mean_test = 0.5408\n",
      "\n",
      "Results for 1000 best features\n",
      "Train scores: [0.66787554 0.67396278 0.67286435 0.66773249 0.66925989] -> mean_train = 0.6703\n",
      "Test scores: [0.54568328 0.52206459 0.53540264 0.55007844 0.54338797] -> mean_test = 0.5393\n",
      "\n",
      "Results for 1100 best features\n",
      "Train scores: [0.69352346 0.6948638  0.68931223 0.69610607 0.68214437] -> mean_train = 0.6912\n",
      "Test scores: [0.57321944 0.57223914 0.59692036 0.55132183 0.55748441] -> mean_test = 0.5702\n",
      "\n",
      "Results for 1200 best features\n",
      "Train scores: [0.69206243 0.69502702 0.69416212 0.69338847 0.6969137 ] -> mean_train = 0.6943\n",
      "Test scores: [0.57839476 0.57089618 0.58192688 0.57475982 0.55218899] -> mean_test = 0.5716\n",
      "\n",
      "Results for 1300 best features\n",
      "Train scores: [0.69576919 0.69370551 0.69591981 0.69425998 0.67655226] -> mean_train = 0.6912\n",
      "Test scores: [0.5726625  0.57750318 0.56965538 0.57275846 0.56328342] -> mean_test = 0.5712\n",
      "\n",
      "Results for 1400 best features\n",
      "Train scores: [0.6912309  0.69517008 0.69510343 0.69311147 0.68308284] -> mean_train = 0.6915\n",
      "Test scores: [0.5802304  0.56588862 0.56999854 0.57888254 0.54718202] -> mean_test = 0.5684\n",
      "\n",
      "Results for 1500 best features\n",
      "Train scores: [0.69648327 0.69624902 0.69995392 0.69291142 0.68118336] -> mean_train = 0.6934\n",
      "Test scores: [0.56878518 0.56883332 0.55616613 0.59378321 0.58255785] -> mean_test = 0.5740\n",
      "\n",
      "Results for 1600 best features\n",
      "Train scores: [0.69645181 0.65683526 0.70009776 0.69207575 0.69738217] -> mean_train = 0.6886\n",
      "Test scores: [0.55683351 0.57472355 0.55793864 0.58730223 0.57331733] -> mean_test = 0.5700\n",
      "\n",
      "Results for 1700 best features\n",
      "Train scores: [0.68402266 0.67682186 0.69298539 0.69737798 0.69693916] -> mean_train = 0.6896\n",
      "Test scores: [0.59166452 0.55001258 0.57317637 0.55063248 0.56945549] -> mean_test = 0.5670\n",
      "\n",
      "Results for 1800 best features\n",
      "Train scores: [0.69636665 0.69543724 0.6926872  0.69620411 0.69621706] -> mean_train = 0.6954\n",
      "Test scores: [0.56216641 0.57699126 0.56875645 0.57260295 0.5738527 ] -> mean_test = 0.5709\n",
      "\n",
      "Results for 1900 best features\n",
      "Train scores: [0.69199944 0.69749367 0.69985996 0.69853    0.69549178] -> mean_train = 0.6967\n",
      "Test scores: [0.5899744  0.56577578 0.5450411  0.5664836  0.57838029] -> mean_test = 0.5691\n",
      "\n",
      "Results for 2000 best features\n",
      "Train scores: [0.69645712 0.67113174 0.69395222 0.69647604 0.6989796 ] -> mean_train = 0.6914\n",
      "Test scores: [0.57832126 0.58382683 0.57307968 0.57548771 0.5519819 ] -> mean_test = 0.5725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_corr_models(model=HGBR(), method=\"Pearson correlation\", corrs=pearson_corrs, num_feats=range(800, 2001, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6297b9a-6268-4433-b3af-6d56498ce72d",
   "metadata": {},
   "source": [
    "Para este método de seleção de features (<b>correlação de Pearson</b>) obtivemos um score máximo de <b>0.5740</b> para a combinação <b>HistGradientBoostingRegressor / 1500 features</b>. No entanto, selecionámos a combinação <b>HistGradientBoostingRegressor / 1200 features </b> por se encontrar numa região mais estável no espaço de procura do número de features ótimo. Neste caso, o score obtido foi <b>0.5716</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aba4df-9491-439c-a2f9-185237f4cac4",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ca67e625-89da-4e16-a524-be56c7afb606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter warnings -> MLPR related\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0042a47-91d1-4508-9b9b-bd500b38bb57",
   "metadata": {},
   "source": [
    "<b>Spearman</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1d76400-cfd1-471f-bee0-c1a2df54e35e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for LinearRegression using Spearman correlation to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.30276032 0.30999642 0.29627182 0.30485768 0.30062867] -> mean_train = 0.3029\n",
      "Test scores: [0.29231614 0.26266536 0.31845467 0.28441096 0.30099507] -> mean_test = 0.2918\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.38744808 0.38428109 0.3826784  0.33979449 0.38501327] -> mean_train = 0.3758\n",
      "Test scores: [0.3538583  0.36657723 0.33417186 0.30842076 0.36367539] -> mean_test = 0.3453\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.41855357 0.41669686 0.41262532 0.41615306 0.42218293] -> mean_train = 0.4172\n",
      "Test scores: [0.37755745 0.38235578 0.39904554 0.39122124 0.34962146] -> mean_test = 0.3800\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.43666678 0.42842225 0.43421242 0.43776558 0.43959521] -> mean_train = 0.4353\n",
      "Test scores: [0.39523955 0.38230352 0.37197668 0.39334337 0.39326577] -> mean_test = 0.3872\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for KNeighborsRegressor using Spearman correlation to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.58144722 0.56791155 0.57262067 0.57422373 0.57555677] -> mean_train = 0.5744\n",
      "Test scores: [0.34116207 0.38116106 0.36167522 0.36244282 0.34761941] -> mean_test = 0.3588\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.60354152 0.60200782 0.6085737  0.60455412 0.59745753] -> mean_train = 0.6032\n",
      "Test scores: [0.41038186 0.41923013 0.36877787 0.39774969 0.43388593] -> mean_test = 0.4060\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.61668517 0.61529299 0.61350762 0.61218549 0.62025389] -> mean_train = 0.6156\n",
      "Test scores: [0.40996733 0.42958298 0.45250813 0.42477277 0.39997428] -> mean_test = 0.4234\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.60642041 0.60735275 0.61471306 0.60815288 0.61213709] -> mean_train = 0.6098\n",
      "Test scores: [0.42858132 0.42051792 0.41496123 0.4170616  0.42208561] -> mean_test = 0.4206\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for RandomForestRegressor using Spearman correlation to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.86286666 0.86327227 0.86515871 0.86526029 0.86058221] -> mean_train = 0.8634\n",
      "Test scores: [0.45806815 0.45428154 0.43739402 0.44168913 0.47137121] -> mean_test = 0.4526\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.86659924 0.86960868 0.8664068  0.87180788 0.86824758] -> mean_train = 0.8685\n",
      "Test scores: [0.50130257 0.48736882 0.50682832 0.46481167 0.47564835] -> mean_test = 0.4872\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.87154669 0.86740715 0.8687975  0.86999956 0.86789773] -> mean_train = 0.8691\n",
      "Test scores: [0.47269618 0.49320745 0.49477457 0.49530748 0.48645884] -> mean_test = 0.4885\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.86992188 0.86863113 0.86980674 0.86922954 0.86885264] -> mean_train = 0.8693\n",
      "Test scores: [0.47346002 0.48232316 0.49540531 0.48162644 0.48545635] -> mean_test = 0.4837\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for SVR using Spearman correlation to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.27542999 0.2723476  0.27631393 0.2772708  0.28241463] -> mean_train = 0.2768\n",
      "Test scores: [0.28283144 0.28564874 0.25955313 0.26985495 0.2524273 ] -> mean_test = 0.2701\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.25050722 0.24563219 0.24316983 0.24466171 0.25483817] -> mean_train = 0.2478\n",
      "Test scores: [0.22647406 0.24715775 0.24966636 0.2546818  0.23105567] -> mean_test = 0.2418\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.24899087 0.24717708 0.24506422 0.24771682 0.24762889] -> mean_train = 0.2473\n",
      "Test scores: [0.24235095 0.22966731 0.26202853 0.2312249  0.24195584] -> mean_test = 0.2414\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.24881949 0.26274852 0.25277209 0.25090909 0.25622489] -> mean_train = 0.2543\n",
      "Test scores: [0.26830012 0.2200019  0.24973791 0.26164309 0.24226996] -> mean_test = 0.2484\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for MLPRegressor using Spearman correlation to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.55277466 0.55306357 0.54210614 0.47871056 0.58166296] -> mean_train = 0.5417\n",
      "Test scores: [0.43527307 0.42055891 0.43856691 0.32758635 0.40498687] -> mean_test = 0.4054\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.52406768 0.53170162 0.55011785 0.52939447 0.50005963] -> mean_train = 0.5271\n",
      "Test scores: [0.47915438 0.47292983 0.48500241 0.46352347 0.45960753] -> mean_test = 0.4720\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.5295686  0.48546583 0.52897515 0.53093441 0.51876645] -> mean_train = 0.5187\n",
      "Test scores: [0.46535184 0.44473846 0.46850541 0.4922603  0.44927276] -> mean_test = 0.4640\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.53871824 0.55747297 0.56545089 0.51894987 0.56946029] -> mean_train = 0.5500\n",
      "Test scores: [0.47679662 0.48825781 0.4961454  0.43920372 0.47527757] -> mean_test = 0.4751\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for AdaBoostRegressor using Spearman correlation to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.27077418 0.28118372 0.28132986 0.27185248 0.28699682] -> mean_train = 0.2784\n",
      "Test scores: [0.29456676 0.27699162 0.2566113  0.24467112 0.27629458] -> mean_test = 0.2698\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.30250501 0.29647762 0.32411476 0.34112609 0.29839447] -> mean_train = 0.3125\n",
      "Test scores: [0.29654511 0.28648838 0.30943204 0.32816846 0.28505545] -> mean_test = 0.3011\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.27707697 0.29286917 0.29149421 0.27581087 0.30896063] -> mean_train = 0.2892\n",
      "Test scores: [0.27402579 0.28327587 0.25670492 0.27295977 0.29208173] -> mean_test = 0.2758\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.29183765 0.2888603  0.30779433 0.28695369 0.31408741] -> mean_train = 0.2979\n",
      "Test scores: [0.25761796 0.29439967 0.29200319 0.28608114 0.30552038] -> mean_test = 0.2871\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for HistGradientBoostingRegressor using Spearman correlation to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.60516408 0.5987729  0.60463261 0.60686439 0.60086074] -> mean_train = 0.6033\n",
      "Test scores: [0.49752306 0.51061865 0.49868967 0.50033296 0.52337996] -> mean_test = 0.5061\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.65131722 0.64860272 0.64927219 0.65068186 0.6526421 ] -> mean_train = 0.6505\n",
      "Test scores: [0.53977152 0.53134494 0.54185351 0.54044321 0.52402806] -> mean_test = 0.5355\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.66156741 0.65808232 0.6616351  0.65506371 0.6288157 ] -> mean_train = 0.6530\n",
      "Test scores: [0.53542535 0.54794698 0.51950571 0.53169638 0.55002214] -> mean_test = 0.5369\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.66139306 0.64139385 0.65894965 0.66316064 0.66545993] -> mean_train = 0.6581\n",
      "Test scores: [0.53807847 0.52731971 0.5485689  0.54767644 0.52776092] -> mean_test = 0.5379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, Model in enumerate([LR, KNR, RFR, SVR, MLPR, ADA, HGBR]):\n",
    "    test_corr_models(model=Model(), method=\"Spearman correlation\", corrs=spearman_corrs)\n",
    "    if i < 6:\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cffe67f-bbe5-47c2-ac2a-fb8ead3b8127",
   "metadata": {},
   "source": [
    "<b>Spearman</b> (further testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb8a42-bbf8-48ce-ba85-155fe0cab687",
   "metadata": {},
   "source": [
    "Novos testes para uma gama de features distinta da anterior. Testes efetuados para o melhor modelo - <b>HistGradientBoostingRegressor</b> - e para o modelo <b>LinearRegression</b> (progresão de scores promissora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "48f496d9-e076-4fcc-b1a1-8c73ff2b59b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for LinearRegression using Spearman correlation to select features...\n",
      "\n",
      "Results for 900 best features\n",
      "Train scores: [0.44845659 0.44690669 0.45188674 0.44772917 0.4460683 ] -> mean_train = 0.4482\n",
      "Test scores: [0.37239857 0.40371682 0.36711783 0.40791929 0.41591596] -> mean_test = 0.3934\n",
      "\n",
      "Results for 1000 best features\n",
      "Train scores: [0.44677465 0.45609028 0.45454997 0.46056612 0.44835278] -> mean_train = 0.4533\n",
      "Test scores: [0.38473902 0.39825773 0.40052927 0.38249102 0.41902439] -> mean_test = 0.3970\n",
      "\n",
      "Results for 1100 best features\n",
      "Train scores: [0.45673772 0.46337917 0.45801098 0.45532306 0.46020887] -> mean_train = 0.4587\n",
      "Test scores: [0.39463812 0.37497419 0.41201434 0.42376544 0.39754653] -> mean_test = 0.4006\n",
      "\n",
      "Results for 1200 best features\n",
      "Train scores: [0.46731002 0.46380396 0.47062831 0.46190505 0.46045568] -> mean_train = 0.4648\n",
      "Test scores: [0.39199155 0.39522075 0.38187573 0.42181252 0.41388638] -> mean_test = 0.4010\n",
      "\n",
      "Results for 1300 best features\n",
      "Train scores: [0.47142267 0.46782679 0.46742981 0.46838148 0.46693459] -> mean_train = 0.4684\n",
      "Test scores: [0.39921763 0.38336105 0.41378528 0.40241595 0.41758455] -> mean_test = 0.4033\n",
      "\n",
      "Results for 1400 best features\n",
      "Train scores: [0.46847459 0.47484548 0.47232039 0.47313792 0.47524504] -> mean_train = 0.4728\n",
      "Test scores: [0.43056256 0.3855252  0.4116635  0.39009259 0.39527261] -> mean_test = 0.4026\n",
      "\n",
      "Results for 1500 best features\n",
      "Train scores: [0.47858775 0.47802305 0.46858288 0.47044577 0.47881881] -> mean_train = 0.4749\n",
      "Test scores: [0.38243632 0.3995922  0.42093709 0.40010573 0.37637554] -> mean_test = 0.3959\n",
      "\n",
      "Results for 1600 best features\n",
      "Train scores: [0.4858436  0.47689532 0.47976781 0.48110309 0.48202328] -> mean_train = 0.4811\n",
      "Test scores: [0.35896603 0.42488088 0.35953264 0.40529942 0.4013409 ] -> mean_test = 0.3900\n",
      "\n",
      "Results for 1700 best features\n",
      "Train scores: [0.48145071 0.48511768 0.48920228 0.48001406 0.48410548] -> mean_train = 0.4840\n",
      "Test scores: [0.41967937 0.39818969 0.39131978 0.41330345 0.3542309 ] -> mean_test = 0.3953\n",
      "\n",
      "Results for 1800 best features\n",
      "Train scores: [0.48788269 0.48986427 0.48178563 0.48823181 0.49033944] -> mean_train = 0.4876\n",
      "Test scores: [0.41527967 0.39016132 0.41973562 0.40783465 0.34565396] -> mean_test = 0.3957\n",
      "\n",
      "Results for 1900 best features\n",
      "Train scores: [0.49636614 0.48962182 0.49556593 0.48604723 0.49170334] -> mean_train = 0.4919\n",
      "Test scores: [0.36662858 0.36986296 0.39454486 0.43095738 0.4014067 ] -> mean_test = 0.3927\n",
      "\n",
      "Results for 2000 best features\n",
      "Train scores: [0.49159278 0.49793264 0.49522585 0.49662057 0.49429215] -> mean_train = 0.4951\n",
      "Test scores: [0.40642492 0.35235637 0.40593204 0.38819664 0.41421353] -> mean_test = 0.3934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_corr_models(model=LR(), method=\"Spearman correlation\", corrs=spearman_corrs, num_feats=range(900, 2001, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96da6235-a27a-4703-9856-6484fd416aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for HistGradientBoostingRegressor using Spearman correlation to select features...\n",
      "\n",
      "Results for 900 best features\n",
      "Train scores: [0.66640621 0.66761633 0.66318797 0.66438972 0.66299885] -> mean_train = 0.6649\n",
      "Test scores: [0.5385035  0.53013342 0.53696804 0.54630403 0.53912704] -> mean_test = 0.5382\n",
      "\n",
      "Results for 1000 best features\n",
      "Train scores: [0.66843968 0.66608647 0.66707098 0.67020146 0.65024564] -> mean_train = 0.6644\n",
      "Test scores: [0.54170773 0.53339188 0.55513566 0.52840985 0.54995223] -> mean_test = 0.5417\n",
      "\n",
      "Results for 1100 best features\n",
      "Train scores: [0.66817317 0.65897502 0.66969823 0.66850961 0.66736228] -> mean_train = 0.6665\n",
      "Test scores: [0.54798365 0.54820868 0.54489859 0.53689083 0.53303332] -> mean_test = 0.5422\n",
      "\n",
      "Results for 1200 best features\n",
      "Train scores: [0.66697576 0.67070654 0.67109184 0.67369563 0.66743542] -> mean_train = 0.6700\n",
      "Test scores: [0.56181496 0.53415526 0.54248205 0.5224809  0.55233844] -> mean_test = 0.5427\n",
      "\n",
      "Results for 1300 best features\n",
      "Train scores: [0.67069024 0.67527102 0.66286378 0.66742903 0.67396175] -> mean_train = 0.6700\n",
      "Test scores: [0.54450774 0.52375611 0.56018647 0.55024704 0.53476782] -> mean_test = 0.5427\n",
      "\n",
      "Results for 1400 best features\n",
      "Train scores: [0.67654191 0.66878745 0.66998537 0.67230986 0.66403637] -> mean_train = 0.6703\n",
      "Test scores: [0.52383477 0.54404493 0.55101852 0.53449349 0.54573617] -> mean_test = 0.5398\n",
      "\n",
      "Results for 1500 best features\n",
      "Train scores: [0.67044806 0.67563318 0.67118479 0.66868364 0.66982355] -> mean_train = 0.6712\n",
      "Test scores: [0.55567347 0.53332231 0.54208241 0.54953324 0.54846705] -> mean_test = 0.5458\n",
      "\n",
      "Results for 1600 best features\n",
      "Train scores: [0.67593141 0.67537575 0.67382011 0.64675993 0.67344937] -> mean_train = 0.6691\n",
      "Test scores: [0.53136766 0.53740612 0.55298813 0.5473727  0.54057805] -> mean_test = 0.5419\n",
      "\n",
      "Results for 1700 best features\n",
      "Train scores: [0.67209529 0.67042095 0.67559447 0.66762051 0.66367423] -> mean_train = 0.6699\n",
      "Test scores: [0.55008033 0.54208193 0.532531   0.54504007 0.52712765] -> mean_test = 0.5394\n",
      "\n",
      "Results for 1800 best features\n",
      "Train scores: [0.66989339 0.62961343 0.67362719 0.65298226 0.67519948] -> mean_train = 0.6603\n",
      "Test scores: [0.53343463 0.54984739 0.54449213 0.54810165 0.52855286] -> mean_test = 0.5409\n",
      "\n",
      "Results for 1900 best features\n",
      "Train scores: [0.6755778  0.66552389 0.67168683 0.67462481 0.65582312] -> mean_train = 0.6686\n",
      "Test scores: [0.53451847 0.53947979 0.54446444 0.53571897 0.54730343] -> mean_test = 0.5403\n",
      "\n",
      "Results for 2000 best features\n",
      "Train scores: [0.67643412 0.67331554 0.67335111 0.67241284 0.67489739] -> mean_train = 0.6741\n",
      "Test scores: [0.53448304 0.55207278 0.54070753 0.53314925 0.54743649] -> mean_test = 0.5416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_corr_models(model=HGBR(), method=\"Spearman correlation\", corrs=spearman_corrs, num_feats=range(900, 2001, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e968f-c580-45da-8aaa-52d8f7ad7d38",
   "metadata": {},
   "source": [
    "Através deste método de seleção de features (<b>correlação de Spearman</b>) obtivemos um score máximo de <b>0.5458</b> para a combinação <b>HistGradientBoostingRegressor / 1500 features</b>. No entanto, mais uma vez, selecionámos a combinação <b>HistGradientBoostingRegressor / 1200 features </b> por se encontrar numa região mais estável no espaço de procura do número de features ótimo. Neste caso, o score obtido foi <b>0.5427</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84aa4a-59ef-4964-b1ec-5b63ff0069c8",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe710b8-4e41-460a-adf9-b0ed082bcedc",
   "metadata": {},
   "source": [
    "<b>Univariate linear regression</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e31188f-10eb-4487-b50c-92d23e3d0d8c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for LinearRegression using univariate linear regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.31373502 0.31453924 0.30417023 0.31123685 0.31630096] -> mean_train = 0.3120\n",
      "Test scores: [0.28783469 0.29779471 0.30135783 0.31145979 0.29140639] -> mean_test = 0.2980\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.39764482 0.39875594 0.39529086 0.39313225 0.39097182] -> mean_train = 0.3952\n",
      "Test scores: [0.36595936 0.36158467 0.37670695 0.37031297 0.38661595] -> mean_test = 0.3722\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.42097787 0.42265563 0.42186017 0.42585712 0.42528314] -> mean_train = 0.4233\n",
      "Test scores: [0.40439878 0.40036036 0.40637312 0.38183613 0.30668828] -> mean_test = 0.3799\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.43038356 0.43612914 0.43978573 0.44180339 0.43237401] -> mean_train = 0.4361\n",
      "Test scores: [0.38657825 0.40212593 0.34410856 0.37866801 0.42163509] -> mean_test = 0.3866\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for KNeighborsRegressor using univariate linear regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.58418731 0.58761806 0.58538494 0.59134022 0.59104524] -> mean_train = 0.5879\n",
      "Test scores: [0.39551114 0.38906299 0.40408989 0.37811526 0.35545967] -> mean_test = 0.3844\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.60920937 0.61170261 0.6108256  0.61009618 0.61126893] -> mean_train = 0.6106\n",
      "Test scores: [0.42115555 0.41674847 0.40392014 0.41160636 0.40854717] -> mean_test = 0.4124\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.60560376 0.62121132 0.61906993 0.62097345 0.62114381] -> mean_train = 0.6176\n",
      "Test scores: [0.47479312 0.426869   0.42171527 0.41947493 0.39440085] -> mean_test = 0.4275\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.61991044 0.62148301 0.62284772 0.62001424 0.61840822] -> mean_train = 0.6205\n",
      "Test scores: [0.42825631 0.42061052 0.4388823  0.41319129 0.44369429] -> mean_test = 0.4289\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for RandomForestRegressor using univariate linear regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.8655547  0.86781916 0.86922104 0.865152   0.86404984] -> mean_train = 0.8664\n",
      "Test scores: [0.46708494 0.4685512  0.45384547 0.47275276 0.47673177] -> mean_test = 0.4678\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.86970501 0.86995028 0.86384063 0.86740251 0.87094993] -> mean_train = 0.8684\n",
      "Test scores: [0.47498573 0.48671501 0.51024889 0.47597489 0.45133407] -> mean_test = 0.4799\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.86849813 0.87306993 0.86734382 0.86760202 0.86648112] -> mean_train = 0.8686\n",
      "Test scores: [0.49520044 0.4626488  0.5081917  0.49469975 0.50113926] -> mean_test = 0.4924\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.86669763 0.86905955 0.873318   0.8691455  0.86850394] -> mean_train = 0.8693\n",
      "Test scores: [0.49135522 0.47788815 0.47668487 0.47902245 0.50758556] -> mean_test = 0.4865\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for SVR using univariate linear regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.27740995 0.27944163 0.28039117 0.27269153 0.27814691] -> mean_train = 0.2776\n",
      "Test scores: [0.26537281 0.27334189 0.26638405 0.28994056 0.26355078] -> mean_test = 0.2717\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.26640452 0.26723274 0.26763332 0.26570439 0.25921702] -> mean_train = 0.2652\n",
      "Test scores: [0.25655023 0.2491282  0.26175801 0.2637441  0.26534464] -> mean_test = 0.2593\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.2633362  0.25717587 0.26141754 0.26372821 0.25696623] -> mean_train = 0.2605\n",
      "Test scores: [0.24922436 0.2637499  0.25923562 0.24439915 0.25890932] -> mean_test = 0.2551\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.25483685 0.25661253 0.25750852 0.2579038  0.25711309] -> mean_train = 0.2568\n",
      "Test scores: [0.25750941 0.24639282 0.25115945 0.24799389 0.25132416] -> mean_test = 0.2509\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for MLPRegressor using univariate linear regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.54430174 0.49224234 0.49570655 0.52271968 0.52896249] -> mean_train = 0.5168\n",
      "Test scores: [0.45825457 0.39281788 0.4275423  0.41115331 0.44533216] -> mean_test = 0.4270\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.44218338 0.47154214 0.46907953 0.50640186 0.52612966] -> mean_train = 0.4831\n",
      "Test scores: [0.41794719 0.40787312 0.46285865 0.4551712  0.4717947 ] -> mean_test = 0.4431\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.50729017 0.50026381 0.48247831 0.52337791 0.53190939] -> mean_train = 0.5091\n",
      "Test scores: [0.43744918 0.46668748 0.46659086 0.46733173 0.47559299] -> mean_test = 0.4627\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.51564951 0.533168   0.5462754  0.51449448 0.52771785] -> mean_train = 0.5275\n",
      "Test scores: [0.46981089 0.48904764 0.47320711 0.4360045  0.47072816] -> mean_test = 0.4678\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for AdaBoostRegressor using univariate linear regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.31135141 0.29206503 0.25498672 0.32811158 0.31501928] -> mean_train = 0.3003\n",
      "Test scores: [0.31578124 0.265545   0.26295658 0.30517282 0.30600072] -> mean_test = 0.2911\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.3124823  0.28744464 0.28341977 0.27703655 0.28536512] -> mean_train = 0.2891\n",
      "Test scores: [0.30490637 0.26035061 0.23991877 0.27369385 0.30046343] -> mean_test = 0.2759\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.29051633 0.30135369 0.29399859 0.28739016 0.30473563] -> mean_train = 0.2956\n",
      "Test scores: [0.25262018 0.27319283 0.31489678 0.29137544 0.28354113] -> mean_test = 0.2831\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.31043962 0.31909498 0.29958588 0.31864839 0.28153108] -> mean_train = 0.3059\n",
      "Test scores: [0.32475499 0.30761267 0.28309899 0.29929982 0.25241269] -> mean_test = 0.2934\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for HistGradientBoostingRegressor using univariate linear regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.62287758 0.61627686 0.6203058  0.61895197 0.6068761 ] -> mean_train = 0.6171\n",
      "Test scores: [0.51206187 0.52476304 0.52843164 0.53115901 0.49411325] -> mean_test = 0.5181\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.64348381 0.65269333 0.65288184 0.64936361 0.65076979] -> mean_train = 0.6498\n",
      "Test scores: [0.55823305 0.50265354 0.52208418 0.53948546 0.52743806] -> mean_test = 0.5300\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.65881413 0.66029417 0.64909058 0.6650785  0.66054107] -> mean_train = 0.6588\n",
      "Test scores: [0.54466073 0.52675804 0.53552745 0.53019041 0.5483749 ] -> mean_test = 0.5371\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.65846428 0.67083182 0.66793144 0.63553435 0.66818127] -> mean_train = 0.6602\n",
      "Test scores: [0.55374652 0.52408791 0.53612751 0.54250362 0.53651148] -> mean_test = 0.5386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, Model in enumerate([LR, KNR, RFR, SVR, MLPR, ADA, HGBR]):\n",
    "    test_corr_models(model=Model(), method=\"univariate linear regression\", corrs=f_values)\n",
    "    if i < 6:\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a881ee85-ead7-4da9-84b2-ffc358724079",
   "metadata": {},
   "source": [
    "<b>Univariate linear regression</b> (further testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c62b4-843f-4ca3-be17-034b4ee0ec31",
   "metadata": {},
   "source": [
    "Novos testes para uma gama de features distinta da anterior (apenas para o melhor modelo - <b>HistGradientBoostingRegressor</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38cb59ae-0a16-4f3e-9090-5e1809a5219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for HistGradientBoostingRegressor using univariate linear regression to select features...\n",
      "\n",
      "Results for 900 best features\n",
      "Train scores: [0.66688224 0.6693543  0.64699053 0.65804901 0.65496276] -> mean_train = 0.6592\n",
      "Test scores: [0.54599391 0.53878915 0.53546626 0.55122983 0.52327278] -> mean_test = 0.5390\n",
      "\n",
      "Results for 1000 best features\n",
      "Train scores: [0.6694098  0.66224539 0.66859652 0.66918313 0.67199177] -> mean_train = 0.6683\n",
      "Test scores: [0.54907084 0.55617924 0.53640704 0.54427348 0.51604045] -> mean_test = 0.5404\n",
      "\n",
      "Results for 1100 best features\n",
      "Train scores: [0.69247604 0.69628291 0.6926902  0.69139486 0.69126861] -> mean_train = 0.6928\n",
      "Test scores: [0.5784869  0.56210165 0.56201292 0.5760861  0.57289338] -> mean_test = 0.5703\n",
      "\n",
      "Results for 1200 best features\n",
      "Train scores: [0.69627107 0.69432081 0.69199877 0.69308179 0.69435136] -> mean_train = 0.6940\n",
      "Test scores: [0.56833448 0.57382256 0.57845046 0.56835063 0.5636445 ] -> mean_test = 0.5705\n",
      "\n",
      "Results for 1300 best features\n",
      "Train scores: [0.69475934 0.69202208 0.6960469  0.69555998 0.69639005] -> mean_train = 0.6950\n",
      "Test scores: [0.56968746 0.58672153 0.55414568 0.57689586 0.56750432] -> mean_test = 0.5710\n",
      "\n",
      "Results for 1400 best features\n",
      "Train scores: [0.69881584 0.69692072 0.69314296 0.69586188 0.69289755] -> mean_train = 0.6955\n",
      "Test scores: [0.55043044 0.5779404  0.57557886 0.5731831  0.57545229] -> mean_test = 0.5705\n",
      "\n",
      "Results for 1500 best features\n",
      "Train scores: [0.68757758 0.69831251 0.69299613 0.69575932 0.69099983] -> mean_train = 0.6931\n",
      "Test scores: [0.55513583 0.55758307 0.58213375 0.56612144 0.5801901 ] -> mean_test = 0.5682\n",
      "\n",
      "Results for 1600 best features\n",
      "Train scores: [0.69680396 0.67875579 0.68808888 0.6935068  0.7016316 ] -> mean_train = 0.6918\n",
      "Test scores: [0.57010867 0.58887448 0.55656807 0.57979911 0.54721794] -> mean_test = 0.5685\n",
      "\n",
      "Results for 1700 best features\n",
      "Train scores: [0.68721915 0.6983206  0.70068564 0.69672682 0.70210271] -> mean_train = 0.6970\n",
      "Test scores: [0.60723235 0.56101476 0.56193151 0.57444668 0.55663242] -> mean_test = 0.5723\n",
      "\n",
      "Results for 1800 best features\n",
      "Train scores: [0.68656108 0.69824925 0.69583443 0.67644838 0.6988139 ] -> mean_train = 0.6912\n",
      "Test scores: [0.57885629 0.56883871 0.58091668 0.57711449 0.55996856] -> mean_test = 0.5731\n",
      "\n",
      "Results for 1900 best features\n",
      "Train scores: [0.69954738 0.69798781 0.69430744 0.69749429 0.6929716 ] -> mean_train = 0.6965\n",
      "Test scores: [0.57454464 0.55854714 0.57742535 0.5581389  0.58210644] -> mean_test = 0.5702\n",
      "\n",
      "Results for 2000 best features\n",
      "Train scores: [0.69979395 0.69530248 0.70005613 0.69404283 0.6946719 ] -> mean_train = 0.6968\n",
      "Test scores: [0.56435615 0.56315425 0.56291767 0.57217159 0.57758536] -> mean_test = 0.5680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_corr_models(model=HGBR(), method=\"univariate linear regression\", corrs=f_values, num_feats=range(900, 2001, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c676af-3da6-4f61-a5ee-9b0524960631",
   "metadata": {},
   "source": [
    "Utilizando os <b>ANOVA f-values</b> como método de seleção de features obtivemos um score máximo de <b>0.5731</b> para a combinação <b>HistGradientBoostingRegressor / 1800 features</b>. Todavia, tal como havia acontecido nos casos anteriores, selecionámos a combinação <b>HistGradientBoostingRegressor / 1300 features </b> por se encontrar numa região mais estável no espaço de procura do número de features ótimo. Neste caso, o score obtido foi <b>0.5710</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212759d7-d4a7-41cd-a8a1-87a3c1ce4479",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc819be-5e0c-4259-b4f9-a9def391667b",
   "metadata": {},
   "source": [
    "<b>Mutual information regression</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2bf73db1-01af-41de-a9f2-481dd3aa1e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for LinearRegression using mutual information regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.29933053 0.30549307 0.30197076 0.30446767 0.29626695] -> mean_train = 0.3015\n",
      "Test scores: [0.2959669  0.2691783  0.28315959 0.27444436 0.30818651] -> mean_test = 0.2862\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.39009861 0.39224099 0.39230819 0.3884802  0.38664143] -> mean_train = 0.3900\n",
      "Test scores: [0.34586822 0.34771363 0.35038695 0.36931395 0.37310897] -> mean_test = 0.3573\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.4271809  0.42857239 0.42821025 0.4340767  0.42882269] -> mean_train = 0.4294\n",
      "Test scores: [0.39660344 0.23434375 0.38580482 0.36781109 0.39394364] -> mean_test = 0.3557\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.45024162 0.44818107 0.44927905 0.44205742 0.45097461] -> mean_train = 0.4481\n",
      "Test scores: [0.38582873 0.3408515  0.34684105 0.4190393  0.38900584] -> mean_test = 0.3763\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for KNeighborsRegressor using mutual information regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.66938194 0.6692286  0.67205625 0.67092688 0.67394699] -> mean_train = 0.6711\n",
      "Test scores: [0.49851846 0.50919087 0.50141949 0.50967624 0.48322933] -> mean_test = 0.5004\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.6649939  0.66388582 0.6675673  0.66380483 0.66774893] -> mean_train = 0.6656\n",
      "Test scores: [0.49177363 0.49484785 0.48735343 0.48916829 0.48379032] -> mean_test = 0.4894\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.66257682 0.65881848 0.65683579 0.65848209 0.6576441 ] -> mean_train = 0.6589\n",
      "Test scores: [0.48440107 0.44303405 0.47002927 0.48236651 0.50191799] -> mean_test = 0.4763\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.64859161 0.65853628 0.65476492 0.64637203 0.64708879] -> mean_train = 0.6511\n",
      "Test scores: [0.46616296 0.43867683 0.46643825 0.4858647  0.48901911] -> mean_test = 0.4692\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for RandomForestRegressor using mutual information regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.89444151 0.89337992 0.89547431 0.89845373 0.89603736] -> mean_train = 0.8956\n",
      "Test scores: [0.53502458 0.55360583 0.54909641 0.53530799 0.54899091] -> mean_test = 0.5444\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.89585105 0.8940862  0.89247588 0.89726787 0.89673408] -> mean_train = 0.8953\n",
      "Test scores: [0.54289553 0.55422491 0.54656744 0.5286691  0.53471027] -> mean_test = 0.5414\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.89460238 0.89840983 0.89471388 0.89379372 0.89348131] -> mean_train = 0.8950\n",
      "Test scores: [0.53805615 0.49215636 0.53282304 0.54405425 0.55358532] -> mean_test = 0.5321\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.89466953 0.89382278 0.89359465 0.89711647 0.89608051] -> mean_train = 0.8951\n",
      "Test scores: [0.50978071 0.53141708 0.53141627 0.51194323 0.53676421] -> mean_test = 0.5243\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for SVR using mutual information regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.21831226 0.22384925 0.22468885 0.22106031 0.23286098] -> mean_train = 0.2242\n",
      "Test scores: [0.22227113 0.22504403 0.22066862 0.22561754 0.20318227] -> mean_test = 0.2194\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.27662049 0.27005039 0.27946139 0.27833096 0.28038266] -> mean_train = 0.2770\n",
      "Test scores: [0.26810613 0.28251338 0.26700299 0.26206192 0.27327793] -> mean_test = 0.2706\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.26095284 0.2599437  0.2742461  0.27089909 0.26437299] -> mean_train = 0.2661\n",
      "Test scores: [0.2656998  0.27016543 0.24706366 0.24770026 0.26133609] -> mean_test = 0.2584\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.25518322 0.26270685 0.25606052 0.25377655 0.25957403] -> mean_train = 0.2575\n",
      "Test scores: [0.24989192 0.23992727 0.24558835 0.26371241 0.25509985] -> mean_test = 0.2508\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for MLPRegressor using mutual information regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.4987827  0.52403325 0.48887852 0.49507846 0.49171286] -> mean_train = 0.4997\n",
      "Test scores: [0.46916664 0.47291492 0.50145108 0.4597296  0.47345257] -> mean_test = 0.4753\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.53748188 0.52034604 0.48873221 0.49153429 0.51736356] -> mean_train = 0.5111\n",
      "Test scores: [0.47288314 0.49475383 0.43479143 0.46580252 0.48696947] -> mean_test = 0.4710\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.52394406 0.50311842 0.51250483 0.48896357 0.50551634] -> mean_train = 0.5068\n",
      "Test scores: [0.48286718 0.48485679 0.46718476 0.44797404 0.46836286] -> mean_test = 0.4702\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.49514453 0.49211496 0.49850836 0.49249767 0.48718663] -> mean_train = 0.4931\n",
      "Test scores: [0.44881383 0.46495059 0.46009689 0.46278661 0.4584256 ] -> mean_test = 0.4590\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for AdaBoostRegressor using mutual information regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.30779509 0.27707164 0.29471665 0.32587769 0.31276764] -> mean_train = 0.3036\n",
      "Test scores: [0.29247165 0.2671752  0.29446766 0.31134919 0.29826194] -> mean_test = 0.2927\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.28359559 0.32371424 0.31099812 0.29702389 0.28263655] -> mean_train = 0.2996\n",
      "Test scores: [0.29590716 0.29494945 0.29787745 0.27476386 0.28179445] -> mean_test = 0.2891\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.27976414 0.32424724 0.31334374 0.28116491 0.3159436 ] -> mean_train = 0.3029\n",
      "Test scores: [0.27701969 0.29591232 0.31300413 0.27438443 0.30041012] -> mean_test = 0.2921\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.29247856 0.29151977 0.29933653 0.28316984 0.2901339 ] -> mean_train = 0.2913\n",
      "Test scores: [0.25972989 0.27607238 0.293455   0.2808207  0.28542952] -> mean_test = 0.2791\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Getting results for HistGradientBoostingRegressor using mutual information regression to select features...\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.68981372 0.69006714 0.6844912  0.69081417 0.69452469] -> mean_train = 0.6899\n",
      "Test scores: [0.58052739 0.57938755 0.60418372 0.58680193 0.57019993] -> mean_test = 0.5842\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.69877871 0.69744662 0.69532888 0.70010964 0.69522936] -> mean_train = 0.6974\n",
      "Test scores: [0.58736675 0.58846537 0.60117214 0.56251592 0.58267537] -> mean_test = 0.5844\n",
      "\n",
      "Results for 600 best features\n",
      "Train scores: [0.69906142 0.70169151 0.70439615 0.69289322 0.70444951] -> mean_train = 0.7005\n",
      "Test scores: [0.59873747 0.59653477 0.56286187 0.58374117 0.57587304] -> mean_test = 0.5835\n",
      "\n",
      "Results for 800 best features\n",
      "Train scores: [0.70322891 0.69916378 0.70634854 0.70964569 0.70509382] -> mean_train = 0.7047\n",
      "Test scores: [0.58508506 0.60112711 0.57402897 0.56497576 0.5867018 ] -> mean_test = 0.5824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, Model in enumerate([LR, KNR, RFR, SVR, MLPR, ADA, HGBR]):\n",
    "    test_corr_models(model=Model(), method=\"mutual information regression\", corrs=mutual_info)\n",
    "    if i < 6:\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681809a3-2831-4d4d-801b-490e0c0a5270",
   "metadata": {},
   "source": [
    "<b>Mutual information regression</b> (further testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe62b40-9818-40c4-91eb-e2c04c6a03cd",
   "metadata": {},
   "source": [
    "Obtivemos os melhores scores utilizando o modelo <b>HistGradientBoosting</b> e um número de features entre <b>200</b> e <b>400</b>. Então, efetuámos novas validações cruzadas do modelo tendo em conta números de features pertecentes a esta gama de valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "29e30cd4-e1ae-4515-9304-a42f364e75e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for HistGradientBoostingRegressor using mutual information regression to select features...\n",
      "\n",
      "Results for 100 best features\n",
      "Train scores: [0.64638056 0.647755   0.65273351 0.64679615 0.64730562] -> mean_train = 0.6482\n",
      "Test scores: [0.56075003 0.54913101 0.52380878 0.56463768 0.54638954] -> mean_test = 0.5489\n",
      "\n",
      "Results for 150 best features\n",
      "Train scores: [0.68533162 0.68011919 0.68446605 0.68878994 0.68179735] -> mean_train = 0.6841\n",
      "Test scores: [0.59444313 0.58825639 0.5846732  0.56242807 0.61195944] -> mean_test = 0.5884\n",
      "\n",
      "Results for 200 best features\n",
      "Train scores: [0.69143393 0.68617854 0.68891249 0.689917   0.69075993] -> mean_train = 0.6894\n",
      "Test scores: [0.58727969 0.59127946 0.58798012 0.58635699 0.58367   ] -> mean_test = 0.5873\n",
      "\n",
      "Results for 250 best features\n",
      "Train scores: [0.68964904 0.69257095 0.69232035 0.69044551 0.69519141] -> mean_train = 0.6920\n",
      "Test scores: [0.5855083  0.5921357  0.58939877 0.59280559 0.57684508] -> mean_test = 0.5873\n",
      "\n",
      "Results for 300 best features\n",
      "Train scores: [0.69317032 0.69305788 0.6969709  0.69497039 0.69406247] -> mean_train = 0.6944\n",
      "Test scores: [0.59697292 0.58954455 0.57880944 0.58565545 0.58374544] -> mean_test = 0.5869\n",
      "\n",
      "Results for 350 best features\n",
      "Train scores: [0.69772839 0.69377324 0.70152373 0.69195493 0.69701042] -> mean_train = 0.6964\n",
      "Test scores: [0.58113254 0.60130415 0.57293537 0.5932389  0.58947037] -> mean_test = 0.5876\n",
      "\n",
      "Results for 400 best features\n",
      "Train scores: [0.69333979 0.6969914  0.69768763 0.699293   0.70044102] -> mean_train = 0.6976\n",
      "Test scores: [0.60967112 0.58794547 0.58419745 0.58313238 0.56278805] -> mean_test = 0.5855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_corr_models(model=HGBR(), method=\"mutual information regression\", corrs=mutual_info, num_feats=range(100, 401, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d8280-8048-4111-8095-0e9eea5d85d7",
   "metadata": {},
   "source": [
    "Utilizando a <b>informação mútua</b> como método de seleção de features obtivemos um score máximo de <b>0.5884</b> para a combinação <b>HistGradientBoostingRegressor / 150 features</b>. No entanto, tal como se sucedeu em todos os casos anteriores, selecionámos a combinação <b>HistGradientBoostingRegressor / 200 features </b> por se encontrar numa região mais estável no espaço de procura do número de features ótimo. Neste caso, o score obtido foi <b>0.5873</b>, o melhor resultado até ao momento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b442145-4863-4def-a02f-9b21e9e779e8",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97ed228-585d-49d0-9a08-a1ff5b767044",
   "metadata": {},
   "source": [
    "<b>SelectFromModel</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b858e-b3c2-46a2-8bc9-491e59e5ecf9",
   "metadata": {},
   "source": [
    "De seguida, de modo a testar outro método de seleção de features, utilizámos a classe <b>SelectFromModel</b> do <b>sklearn</b>. A seleção de features é realizada tendo por base o modelo <b>RandomForestRegressor</b> já que, a par do modelo <b>LinearRegression</b>, é o único que apresenta o atributo <b>feature_importances_</b> (<b>coef_</b> no caso do modelo <b>LinearRegression</b>), necessário para a definição de quais features manter e eliminar. As features selecionadas são depois partilhadas na definição do dataset que alimenta os restantes modelos de machine learning (<b>LR</b>, <b>KNR</b>, <b>SVR</b>, <b>MLPR</b>, <b>ADA</b>, <b>HGBR</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b0d066a-b075-48f2-b362-32e56bcbe079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b8171c51",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def cv_select_from_model(models: list, cv=5):\n",
    "    \"\"\"\n",
    "    Cross-validates models using features outputed by sklearn's SelectFromModel using a Random Forest\n",
    "    Regressor as estimator. Returns the computed feature mask for further use.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models: list\n",
    "        A list object containing uninitialized sklearn models\n",
    "    cv: int (default=5)\n",
    "        Number of folds used in cross-validation\n",
    "    \"\"\"\n",
    "    # select best features according to RFR feature importances\n",
    "    # features whose absolute importance value is greater or equal to the mean importance are kept\n",
    "    selector = SelectFromModel(estimator=RFR())\n",
    "    selector.fit(X_train_sc, y_train)\n",
    "    feature_mask = selector.get_support()\n",
    "    # new dataframe containing the features selected by SelectFromModel\n",
    "    X_train_new = X_train_sc.iloc[:, feature_mask]\n",
    "    # iterate through models and cross-validate\n",
    "    for Model in models:\n",
    "        InitModel = Model()\n",
    "        print(f\"Getting results for {InitModel.__class__.__name__} using 'SelectFromModel' to select features...\\n\")\n",
    "        # cross-validate\n",
    "        result = cross_validate(estimator=InitModel,\n",
    "                                X=X_train_new,\n",
    "                                y=y_train,\n",
    "                                cv=KFold(n_splits=cv, shuffle=True),\n",
    "                                return_train_score=True)\n",
    "        # print cross-validation results\n",
    "        mean_train = np.sum(result[\"train_score\"]) / cv\n",
    "        mean_test = np.sum(result[\"test_score\"]) / cv\n",
    "        print(f\"Train scores: {result['train_score']} -> {mean_train = :.4f}\")\n",
    "        print(f\"Test scores: {result['test_score']} -> {mean_test = :.4f}\\n\")\n",
    "    return feature_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f6d1fc3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for LinearRegression using 'SelectFromModel' to select features...\n",
      "\n",
      "Train scores: [0.47917286 0.4787124  0.47911779 0.47851584 0.47531577] -> mean_train = 0.4782\n",
      "Test scores: [0.3846648  0.41275611 0.3885643  0.38054965 0.4225524 ] -> mean_test = 0.3978\n",
      "\n",
      "Getting results for KNeighborsRegressor using 'SelectFromModel' to select features...\n",
      "\n",
      "Train scores: [0.64117131 0.64691899 0.65368522 0.64261927 0.65363452] -> mean_train = 0.6476\n",
      "Test scores: [0.48092228 0.4558046  0.45069206 0.48195009 0.43438383] -> mean_test = 0.4608\n",
      "\n",
      "Getting results for RandomForestRegressor using 'SelectFromModel' to select features...\n",
      "\n",
      "Train scores: [0.89198975 0.8930869  0.89418728 0.89629146 0.89215876] -> mean_train = 0.8935\n",
      "Test scores: [0.53534368 0.5273811  0.49640657 0.51720304 0.52155594] -> mean_test = 0.5196\n",
      "\n",
      "Getting results for SVR using 'SelectFromModel' to select features...\n",
      "\n",
      "Train scores: [0.2441218  0.23882408 0.2410639  0.24465293 0.24739255] -> mean_train = 0.2432\n",
      "Test scores: [0.23128662 0.24998816 0.24070777 0.22920134 0.23454579] -> mean_test = 0.2371\n",
      "\n",
      "Getting results for MLPRegressor using 'SelectFromModel' to select features...\n",
      "\n",
      "Train scores: [0.42067463 0.42944524 0.51796859 0.41438029 0.42415684] -> mean_train = 0.4413\n",
      "Test scores: [0.40686397 0.38491727 0.48272257 0.40023052 0.36919241] -> mean_test = 0.4088\n",
      "\n",
      "Getting results for AdaBoostRegressor using 'SelectFromModel' to select features...\n",
      "\n",
      "Train scores: [0.28959696 0.30083703 0.29143091 0.2909287  0.28061531] -> mean_train = 0.2907\n",
      "Test scores: [0.25596531 0.29722519 0.29148558 0.27467346 0.2703121 ] -> mean_test = 0.2779\n",
      "\n",
      "Getting results for HistGradientBoostingRegressor using 'SelectFromModel' to select features...\n",
      "\n",
      "Train scores: [0.68981507 0.69744861 0.70596076 0.71203651 0.70741611] -> mean_train = 0.7025\n",
      "Test scores: [0.58028429 0.57268925 0.57793106 0.56851494 0.57161777] -> mean_test = 0.5742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_mask = cv_select_from_model(models=[LR, KNR, RFR, SVR, MLPR, ADA, HGBR])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4cb32b-2944-4f19-af17-6474856d60ce",
   "metadata": {},
   "source": [
    "Os resultados sugerem que o melhor modelo é o <b>HistGradientBoostingRegressor</b> com um score de <b>0.5742</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffa6aa4-461c-4552-8179-4b77aaded534",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8a5a8-9432-4e62-b43a-0935fcf819f3",
   "metadata": {},
   "source": [
    "<b>Pearson + Spearman + mutual information</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Na tentativa de melhorar ligeiramente o nosso melhor modelo (combinação <b>HistGradientBoostingRegressor</b> / <b>200 features</b> utilizando a <b>informação mútua</b> como método de seleção de features), decidimos combinar as melhores features selecionadas a partir de cada método de seleção (com exceção dos <b>ANOVA f-values</b>). Para isso, definimos um lower bound de <b>150</b> features e um upper bound de <b>350</b> features, selecionámos as melhores features tendo em conta cada método de seleção e combinámo-las através da disjunção dos nomes associados às mesma. Finalmente, efetuámos novas validações cruzadas 5-fold a partir dos datasets resultantes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating HistGradientBoostingRegressor using 305 (150 comb) features...\n",
      "Train scores: [0.68301761 0.69117031 0.68361854 0.69151876 0.68589539] -> mean_train = 0.6870\n",
      "Test scores: [0.60715197 0.56753502 0.59320281 0.57107169 0.5921822 ] -> mean_test = 0.5862\n",
      "\n",
      "Cross-validating HistGradientBoostingRegressor using 404 (200 comb) features...\n",
      "Train scores: [0.6932829  0.68857836 0.69084999 0.69144631 0.6913527 ] -> mean_train = 0.6911\n",
      "Test scores: [0.58061507 0.58997917 0.59100267 0.5860717  0.58747708] -> mean_test = 0.5870\n",
      "\n",
      "Cross-validating HistGradientBoostingRegressor using 498 (250 comb) features...\n",
      "Train scores: [0.69726495 0.69041586 0.69251149 0.69384081 0.69802487] -> mean_train = 0.6944\n",
      "Test scores: [0.57633606 0.60185842 0.59790227 0.58521013 0.57564917] -> mean_test = 0.5874\n",
      "\n",
      "Cross-validating HistGradientBoostingRegressor using 588 (300 comb) features...\n",
      "Train scores: [0.69776227 0.69850622 0.69715715 0.69354673 0.6991712 ] -> mean_train = 0.6972\n",
      "Test scores: [0.58759311 0.57259427 0.57885996 0.60668513 0.58010628] -> mean_test = 0.5852\n",
      "\n",
      "Cross-validating HistGradientBoostingRegressor using 678 (350 comb) features...\n",
      "Train scores: [0.69871638 0.69956833 0.69968343 0.69848404 0.70071811] -> mean_train = 0.6994\n",
      "Test scores: [0.58591725 0.5735554  0.58815685 0.59272155 0.57522353] -> mean_test = 0.5831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num in [150, 200, 250, 300, 350]:\n",
    "    best_pearson = list(get_k_best_corrs(num, pearson_corrs).keys())\n",
    "    best_spearman = list(get_k_best_corrs(num, spearman_corrs).keys())\n",
    "    best_mutual_info = list(get_k_best_corrs(num, mutual_info).keys())\n",
    "    best_feats = list(set(best_pearson + best_spearman + best_mutual_info))\n",
    "    print(f\"Cross-validating HistGradientBoostingRegressor using {len(best_feats)} ({num} comb) features...\")\n",
    "    x_train_psmi = X_train_sc[best_feats]\n",
    "    result = cross_validate(estimator=HGBR(),\n",
    "                            X=x_train_psmi,\n",
    "                            y=y_train,\n",
    "                            cv=KFold(n_splits=5, shuffle=True),\n",
    "                            return_train_score=True)\n",
    "    mean_train = np.sum(result[\"train_score\"]) / 5\n",
    "    mean_test = np.sum(result[\"test_score\"]) / 5\n",
    "    print(f\"Train scores: {result['train_score']} -> {mean_train = :.4f}\")\n",
    "    print(f\"Test scores: {result['test_score']} -> {mean_test = :.4f}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Os resultados sugerem que o número ótimo de features (tendo em consideração o espaço de procura estabelecido) é <b>250</b> para cada método de seleção. Após disjunção dos nomes das features resultantes da seleção por cada um dos métodos, obtivemos um total de <b>498</b> features e um score médio de teste na validação cruzada 5-fold de <b>0.5874</b>."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "58d8963c",
   "metadata": {},
   "source": [
    "### Optimize hyperparameters of the best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# best number of features considering the combination of methods (from {200, 250, 300, 350, 400})\n",
    "best_k = 250\n",
    "best_pearson = list(get_k_best_corrs(best_k, pearson_corrs).keys())\n",
    "best_spearman = list(get_k_best_corrs(best_k, spearman_corrs).keys())\n",
    "best_mutual_info = list(get_k_best_corrs(best_k, mutual_info).keys())\n",
    "\n",
    "best_features_psmi = list(set(best_pearson + best_spearman + best_mutual_info))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tendo em consideração os melhores modelos para cada método de seleção de features (<b>Pearson</b>, <b>Spearman</b>, <b>ANOVA f-values</b>, <b>informação mútua</b>, <b>SelectFromModel</b> e combinação <b>Pearson + Spearman + informação mútua</b>), procedemos à otimização dos seus hiperparâmetros. Invariavelmente, o melhor modelo foi o <b>HistGradientBoostingRegressor</b>. Consequentemente, apenas definimos um espaço de procura para a otimização de hiperparâmetros para este modelo. Considerámos os seguintes hiperparâmetros: <b>learning_rate</b>, <b>max_iter</b>, <b>max_leaf_nodes</b>, <b>min_samples_leaf</b> e <b>warm_start</b>. De modo a efetuar a procura dos mesmos, recorremos à classe <b>RandomizedSearchCV</b> do <b>sklearn</b>. Não é utilizada uma procura exaustiva (por exemplo, utilizando a classe <b>GridSearchCV</b> do <b>sklearn</b>), já que utilizando a grelha de hiperparâmetros definida em baixo seriam treinados <b>2500</b> modelos (<b>500</b> combinações de hiperparâmetros * <b>5 folds</b> na validação cruzada) para cada método de seleção de features, tornando a complexidade temporal do problema bastante elevada."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93c2b041",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f4eeb9e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "HYPER = {\"learning_rate\": [0.05, 0.1],\n",
    "         \"max_iter\": np.arange(100, 301, 50),\n",
    "         \"max_leaf_nodes\": np.arange(31, 64, 8),\n",
    "         \"min_samples_leaf\": np.arange(16, 33, 4),\n",
    "         \"warm_start\": [True, False]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef53bf7-e4b6-47b7-9256-75d069b2b6bb",
   "metadata": {},
   "source": [
    "<b>Pearson, Spearman, univariate linear regression and mutual information regression</b>\n",
    "<br>(hyperparameter optimization using the above methods to select features to train the models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76344fba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def grid_search(models:list, methods:list, corrs:list, num_feats:list, cv=5):\n",
    "    results = []\n",
    "    for mo, me, co, nf in zip(models, methods, corrs, num_feats):\n",
    "        InitModel = mo()\n",
    "        print(f\"Optimizing {InitModel.__class__.__name__} using {me} to select features ({nf})...\")\n",
    "        print(\"----------\")\n",
    "        x_train_gs = X_train_sc.loc[:, get_k_best_corrs(nf, co).keys()]\n",
    "        gs = RandomizedSearchCV(estimator=InitModel,\n",
    "                                param_distributions=HYPER,\n",
    "                                n_iter=40,\n",
    "                                cv=KFold(n_splits=cv, shuffle=True),\n",
    "                                verbose=3)\n",
    "        gs.fit(x_train_gs, y_train)\n",
    "        results.append(gs.best_params_)\n",
    "        print(\"----------\")\n",
    "        print(f\"Optimal hyperparameters: {gs.best_params_}\")\n",
    "        print(f\"Best score: {gs.best_score_}\\n\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cbba3e9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "models = [HGBR, HGBR, HGBR, HGBR]\n",
    "methods = [\"Pearson correlation\", \"Spearman correlation\", \"univariate linear regression\", \"mutual information regression\"]\n",
    "corrs = [pearson_corrs, spearman_corrs, f_values, mutual_info]\n",
    "num_feats = [1200, 1200, 1300, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9186a6ea-e3de-48e4-a4a9-7331eec3fda6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params = grid_search(models, methods, corrs, num_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46be6d07-34ed-4628-8de1-d48f3cc47d26",
   "metadata": {},
   "source": [
    "Os hiperparâmetros ótimos para cada modelo foram, então, guardados numa variável <b>best_params</b> de modo a realizar nova validação cruzada de cada um dos modelos (nesta ocasião, treinados com hiperparâmetros otimizados)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4189769-b6e7-4bf0-b137-861926c44d1c",
   "metadata": {},
   "source": [
    "<b>SelectFromModel</b>\n",
    "<br>(hyperparameter optmization using SelectFromModel to select the best features to train the models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33a9874a-cd63-4ce6-a19b-38509293f096",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Optimizing HistGradientBoostingRegressor using SelectFromModel to select features...\")\n",
    "print(\"----------\")\n",
    "x_train_sfm = X_train_sc.iloc[:, feature_mask]\n",
    "gs_sfm = RandomizedSearchCV(estimator=HGBR(),\n",
    "                            param_distributions=HYPER,\n",
    "                            n_iter=40,\n",
    "                            cv=KFold(n_splits=5, shuffle=True),\n",
    "                            verbose=3)\n",
    "gs_sfm.fit(x_train_sfm, y_train)\n",
    "print(\"----------\")\n",
    "print(f\"Optimal hyperparameters: {gs_sfm.best_params_}\")\n",
    "print(f\"Best score: {gs_sfm.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d972f740-8107-4c50-b221-f558b9319eae",
   "metadata": {},
   "source": [
    "<b>Pearson + Spearman + mutual information</b>\n",
    "<br>(hyperparameter optmization using a combination of feature selection methods to select the best features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b88c56-e926-4a48-967d-a7f010abfe03",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Optimizing HistGradientBoostingRegressor using a combination of feature selection methods...\")\n",
    "print(\"----------\")\n",
    "x_train_psmi = X_train_sc[best_features_psmi]\n",
    "gs_psmi = RandomizedSearchCV(estimator=HGBR(),\n",
    "                             param_distributions=HYPER,\n",
    "                             n_iter=40,\n",
    "                             cv=KFold(n_splits=5, shuffle=True),\n",
    "                             verbose=3)\n",
    "gs_psmi.fit(x_train_psmi, y_train)\n",
    "print(\"----------\")\n",
    "print(f\"Optimal hyperparameters: {gs_psmi.best_params_}\")\n",
    "print(f\"Best score: {gs_psmi.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a050d6e-23c5-433c-b31d-c8525071ec60",
   "metadata": {},
   "source": [
    "Os hiperparâmetros ótimos referentes aos modelos obtidos pelos restantes métodos de seleção de features serão futuramente acessados através do atributo <b>best_params_</b> de objetos da classe <b>RandomizedSearchCV</b> (<b>gs_sfm.best_params_</b> e <b>gs_psmi.best_params_</b>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78576f",
   "metadata": {},
   "source": [
    "### Cross-validate best models (with optimized hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262d73b7-45c9-4b93-83de-225b313d0e86",
   "metadata": {},
   "source": [
    "Procedemos, então, a uma validação cruzada 10-fold dos melhores modelos obtidos através de cada um dos métodos de seleção de features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a6081-a6df-4d98-8e6d-92a41fc35ce6",
   "metadata": {},
   "source": [
    "<b>Pearson, Spearman, univariate linear regression and mutual information regression</b>\n",
    "<br>(cross-validation of the best models with optimized hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "f3d9df1f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# only cross-validates HistGradientBoostingRegressor, as it was invariably the best model\n",
    "def cv_best(models, params: list, methods: list, corrs: list, num_feats: list, cv=10):\n",
    "    for mo, pr, me, co, nf in zip(models, params, methods, corrs, num_feats):\n",
    "        InitModel = mo()\n",
    "        print(f\"Cross-validating ({cv}-fold) {InitModel.__class__.__name__} using {me} ({nf} features)...\")\n",
    "        print(f\"Optimized hyperparameters: {pr}\")\n",
    "        x_train = X_train_sc.loc[:,get_k_best_corrs(nf, co).keys()]\n",
    "        result = cross_validate(estimator=InitModel.set_params(**pr),\n",
    "                                X=x_train,\n",
    "                                y=y_train,\n",
    "                                cv=KFold(n_splits=cv, shuffle=True),\n",
    "                                return_train_score=True)\n",
    "        mean_train = np.sum(result[\"train_score\"]) / cv\n",
    "        mean_test = np.sum(result[\"test_score\"]) / cv\n",
    "        print(f\"Train scores: {result['train_score']} -> {mean_train = :.4f}\")\n",
    "        print(f\"Test scores: {result['test_score']} -> {mean_test = :.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "e8281b35",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating (10-fold) HistGradientBoostingRegressor using Pearson correlation (1200 features)...\n",
      "Optimized hyperparameters: {'warm_start': False, 'min_samples_leaf': 20, 'max_leaf_nodes': 63, 'max_iter': 300, 'learning_rate': 0.05}\n",
      "Train scores: [0.73233652 0.73407498 0.73019125 0.74800317 0.73221829 0.71548573\n",
      " 0.75295105 0.74165818 0.73646702 0.7178306 ] -> mean_train = 0.7341\n",
      "Test scores: [0.56623301 0.60612089 0.57279343 0.57305721 0.56753691 0.60907319\n",
      " 0.59046368 0.57254022 0.61033073 0.58555759] -> mean_test = 0.5854\n",
      "\n",
      "Cross-validating (10-fold) HistGradientBoostingRegressor using Spearman correlation (1200 features)...\n",
      "Optimized hyperparameters: {'warm_start': False, 'min_samples_leaf': 28, 'max_leaf_nodes': 63, 'max_iter': 250, 'learning_rate': 0.05}\n",
      "Train scores: [0.7046133  0.72345338 0.68532331 0.74236339 0.72630959 0.69784186\n",
      " 0.68999517 0.73570951 0.72595013 0.71897368] -> mean_train = 0.7151\n",
      "Test scores: [0.53102774 0.55152941 0.55988561 0.55823386 0.5664989  0.55229307\n",
      " 0.58424717 0.56240558 0.52764792 0.54849861] -> mean_test = 0.5542\n",
      "\n",
      "Cross-validating (10-fold) HistGradientBoostingRegressor using univariate linear regression (1300 features)...\n",
      "Optimized hyperparameters: {'warm_start': False, 'min_samples_leaf': 28, 'max_leaf_nodes': 63, 'max_iter': 250, 'learning_rate': 0.05}\n",
      "Train scores: [0.75582173 0.7276507  0.76533005 0.72540476 0.73508961 0.73966701\n",
      " 0.74035541 0.73509884 0.73865676 0.76293388] -> mean_train = 0.7426\n",
      "Test scores: [0.59155216 0.5764809  0.60739257 0.5594956  0.59567879 0.59699155\n",
      " 0.58574327 0.59043057 0.58562394 0.58767947] -> mean_test = 0.5877\n",
      "\n",
      "Cross-validating (10-fold) HistGradientBoostingRegressor using mutual information regression (200 features)...\n",
      "Optimized hyperparameters: {'warm_start': False, 'min_samples_leaf': 32, 'max_leaf_nodes': 63, 'max_iter': 300, 'learning_rate': 0.05}\n",
      "Train scores: [0.76875166 0.73104154 0.75049263 0.76938016 0.76481907 0.75684443\n",
      " 0.73644814 0.74662982 0.76163698 0.78642204] -> mean_train = 0.7572\n",
      "Test scores: [0.60509896 0.6142814  0.60063617 0.63996885 0.62272178 0.59556783\n",
      " 0.60761713 0.59120005 0.59343065 0.58970676] -> mean_test = 0.6060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all variables defined above (models, best_params, methods, corrs, num_feats)\n",
    "cv_best(models, best_params, methods, corrs, num_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5695b43f-643c-4d8b-970d-07261b01c51f",
   "metadata": {},
   "source": [
    "Os resultados da validação cruzada demonstram que o melhor score foi obtido utilizando a <b>informação mútua</b> como método de seleção de features (<b>0.6060</b>). O modelo respetivo será, então comparado com os modelos obtidos a partir dos métodos de seleção de features <b>SelectFromModel</b> e <b>Pearson + Spearman + mutual information</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d3a0c-c3b5-4b8f-b634-2017b325da91",
   "metadata": {},
   "source": [
    "<b>SelectFromModel</b>\n",
    "<br>(cross-validation of the best model with optimized hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "f685b26d-2c39-42f4-bf57-d9c211140ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating (10-fold) HistGradientBoostingRegressor using SelectFromModel to select features...\n",
      "Optimized hyperparameters: {'warm_start': False, 'min_samples_leaf': 32, 'max_leaf_nodes': 55, 'max_iter': 200, 'learning_rate': 0.05}\n",
      "Train scores: [0.72930953 0.72559338 0.72952691 0.73186194 0.73014827 0.7371733\n",
      " 0.71938107 0.72341578 0.75435627 0.76341954] -> mean_train = 0.7344\n",
      "Test scores: [0.57799925 0.62441839 0.60640188 0.57680143 0.60326652 0.59459631\n",
      " 0.60107882 0.56927381 0.58052564 0.54540325] -> mean_test = 0.5880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cross-validating (10-fold) HistGradientBoostingRegressor using SelectFromModel to select features...\")\n",
    "print(f\"Optimized hyperparameters: {gs_sfm.best_params_}\")\n",
    "x_train_sfm = X_train_sc.iloc[:, feature_mask]\n",
    "result = cross_validate(estimator=HGBR(**gs_sfm.best_params_),\n",
    "                        X=x_train_sfm,\n",
    "                        y=y_train,\n",
    "                        cv=KFold(n_splits=10, shuffle=True),\n",
    "                        return_train_score=True)\n",
    "mean_train = np.sum(result[\"train_score\"]) / 10\n",
    "mean_test = np.sum(result[\"test_score\"]) / 10\n",
    "print(f\"Train scores: {result['train_score']} -> {mean_train = :.4f}\")\n",
    "print(f\"Test scores: {result['test_score']} -> {mean_test = :.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba12b6-b1d5-4fb3-8ba8-5052acc92014",
   "metadata": {},
   "source": [
    "O score obtido na validação cruzada foi de <b>0.5880</b>. Sendo assim, o melhor modelo até ao momento é o obtido através da seleção de features pela <b>informação mútua</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d3dd86-518a-424c-9b3c-7dc45c8e529f",
   "metadata": {},
   "source": [
    "<b>Pearson + Spearman + mutual information</b>\n",
    "<br>(cross-validation of the best model with optimized hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4716ec1-3f24-416c-9efe-1bafa8fa691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validating (10-fold) HistGradientBoostingRegressor using a combination of feature selection methods...\n",
      "Optimized hyperparameters: {'warm_start': True, 'min_samples_leaf': 32, 'max_leaf_nodes': 63, 'max_iter': 300, 'learning_rate': 0.05}\n",
      "Train scores: [0.75380586 0.74244005 0.74206205 0.74833658 0.74580747 0.75685111\n",
      " 0.74250112 0.74689319 0.75487387 0.74426017] -> mean_train = 0.7478\n",
      "Test scores: [0.58764092 0.59907974 0.62072532 0.62004219 0.60299351 0.60183481\n",
      " 0.60394989 0.59682932 0.58152845 0.59126593] -> mean_test = 0.6006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cross-validating (10-fold) HistGradientBoostingRegressor using a combination of feature selection methods...\")\n",
    "print(f\"Optimized hyperparameters: {gs_psmi.best_params_}\")\n",
    "x_train_psmi = X_train_sc[best_features_psmi]\n",
    "result = cross_validate(estimator=HGBR(**gs_psmi.best_params_),\n",
    "                        X=x_train_psmi,\n",
    "                        y=y_train,\n",
    "                        cv=KFold(n_splits=10, shuffle=True),\n",
    "                        return_train_score=True)\n",
    "mean_train = np.sum(result[\"train_score\"]) / 10\n",
    "mean_test = np.sum(result[\"test_score\"]) / 10\n",
    "print(f\"Train scores: {result['train_score']} -> {mean_train = :.4f}\")\n",
    "print(f\"Test scores: {result['test_score']} -> {mean_test = :.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2d8256-b663-4db0-95f0-e15a5e7e57ae",
   "metadata": {},
   "source": [
    "Finalmente, obteve-se um score médio de <b>0.6015</b> na validaçao cruzada referente ao modelo obtido pela combinação de métodos de seleção de features <b>Pearson + Spearman + mutual information</b>. Verificamos, então, que o melhor modelo se trata do obtido através da seleção de features pela <b>informação mútua</b>, sendo utilizado para efetuar previsões acerca dos dados de teste (sequências de aminoácidos sem label associada)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b7a3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use best model to predict labels in test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "594e7ed7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from feature_extraction import get_dataset_with_features\n",
    "\n",
    "test = pd.read_csv(\"Files/test.csv\")\n",
    "test_data = get_dataset_with_features(test)\n",
    "test_data.to_csv(\"Files/data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aa7958dd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# split features and label\n",
    "X_test = test_data.iloc[:, 2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "45fec257-488b-4fd2-a036-d49fae6ac00e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      SeqLength       A      R      N      D      C      E      Q      G    H  \\\n0           221   9.955  1.357  8.597  6.787  1.810  3.620  5.882  8.597  0.0   \n1           221   9.955  1.357  8.597  6.787  1.810  3.167  5.882  8.597  0.0   \n2           220  10.000  1.364  8.636  6.818  1.818  3.182  5.909  8.636  0.0   \n3           221   9.955  1.357  8.597  6.787  2.262  3.167  5.882  8.597  0.0   \n4           221   9.955  1.357  8.597  6.787  1.810  3.167  5.882  8.597  0.0   \n...         ...     ...    ...    ...    ...    ...    ...    ...    ...  ...   \n2408        221   9.502  1.357  8.597  6.787  1.810  3.167  5.882  8.597  0.0   \n2409        221   9.502  1.357  8.597  6.787  1.810  3.167  5.882  8.597  0.0   \n2410        221   9.502  1.357  9.050  6.787  1.810  3.167  5.882  8.597  0.0   \n2411        221   9.502  1.357  8.597  6.787  1.810  3.167  5.882  8.597  0.0   \n2412        221   9.502  1.357  8.597  6.787  1.810  3.167  5.882  8.597  0.0   \n\n      ...  MoreauBrotoAuto_Mutability22  MoreauBrotoAuto_Mutability23  \\\n0     ...                        -0.021                        -0.025   \n1     ...                        -0.023                        -0.027   \n2     ...                        -0.025                        -0.029   \n3     ...                        -0.022                        -0.026   \n4     ...                        -0.023                        -0.027   \n...   ...                           ...                           ...   \n2408  ...                        -0.023                        -0.027   \n2409  ...                        -0.014                        -0.018   \n2410  ...                        -0.029                        -0.033   \n2411  ...                        -0.017                        -0.021   \n2412  ...                        -0.011                        -0.015   \n\n      MoreauBrotoAuto_Mutability24  MoreauBrotoAuto_Mutability25  \\\n0                           -0.032                        -0.023   \n1                           -0.034                        -0.025   \n2                           -0.035                        -0.027   \n3                           -0.033                        -0.024   \n4                           -0.034                        -0.025   \n...                            ...                           ...   \n2408                        -0.034                        -0.025   \n2409                        -0.025                        -0.016   \n2410                        -0.040                        -0.031   \n2411                        -0.027                        -0.019   \n2412                        -0.021                        -0.013   \n\n      MoreauBrotoAuto_Mutability26  MoreauBrotoAuto_Mutability27  \\\n0                           -0.019                        -0.016   \n1                           -0.021                        -0.018   \n2                           -0.023                        -0.020   \n3                           -0.020                        -0.017   \n4                           -0.021                        -0.018   \n...                            ...                           ...   \n2408                        -0.021                        -0.018   \n2409                        -0.012                        -0.009   \n2410                        -0.028                        -0.024   \n2411                        -0.015                        -0.012   \n2412                        -0.009                        -0.006   \n\n      MoreauBrotoAuto_Mutability28  MoreauBrotoAuto_Mutability29  \\\n0                           -0.019                        -0.026   \n1                           -0.021                        -0.028   \n2                           -0.023                        -0.030   \n3                           -0.020                        -0.027   \n4                           -0.021                        -0.028   \n...                            ...                           ...   \n2408                        -0.021                        -0.028   \n2409                        -0.012                        -0.019   \n2410                        -0.028                        -0.035   \n2411                        -0.015                        -0.022   \n2412                        -0.009                        -0.016   \n\n      MoreauBrotoAuto_Mutability30  pH  \n0                           -0.021   8  \n1                           -0.023   8  \n2                           -0.025   8  \n3                           -0.022   8  \n4                           -0.023   8  \n...                            ...  ..  \n2408                        -0.023   8  \n2409                        -0.014   8  \n2410                        -0.029   8  \n2411                        -0.017   8  \n2412                        -0.010   8  \n\n[2413 rows x 9298 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SeqLength</th>\n      <th>A</th>\n      <th>R</th>\n      <th>N</th>\n      <th>D</th>\n      <th>C</th>\n      <th>E</th>\n      <th>Q</th>\n      <th>G</th>\n      <th>H</th>\n      <th>...</th>\n      <th>MoreauBrotoAuto_Mutability22</th>\n      <th>MoreauBrotoAuto_Mutability23</th>\n      <th>MoreauBrotoAuto_Mutability24</th>\n      <th>MoreauBrotoAuto_Mutability25</th>\n      <th>MoreauBrotoAuto_Mutability26</th>\n      <th>MoreauBrotoAuto_Mutability27</th>\n      <th>MoreauBrotoAuto_Mutability28</th>\n      <th>MoreauBrotoAuto_Mutability29</th>\n      <th>MoreauBrotoAuto_Mutability30</th>\n      <th>pH</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>221</td>\n      <td>9.955</td>\n      <td>1.357</td>\n      <td>8.597</td>\n      <td>6.787</td>\n      <td>1.810</td>\n      <td>3.620</td>\n      <td>5.882</td>\n      <td>8.597</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.021</td>\n      <td>-0.025</td>\n      <td>-0.032</td>\n      <td>-0.023</td>\n      <td>-0.019</td>\n      <td>-0.016</td>\n      <td>-0.019</td>\n      <td>-0.026</td>\n      <td>-0.021</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>221</td>\n      <td>9.955</td>\n      <td>1.357</td>\n      <td>8.597</td>\n      <td>6.787</td>\n      <td>1.810</td>\n      <td>3.167</td>\n      <td>5.882</td>\n      <td>8.597</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.023</td>\n      <td>-0.027</td>\n      <td>-0.034</td>\n      <td>-0.025</td>\n      <td>-0.021</td>\n      <td>-0.018</td>\n      <td>-0.021</td>\n      <td>-0.028</td>\n      <td>-0.023</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>220</td>\n      <td>10.000</td>\n      <td>1.364</td>\n      <td>8.636</td>\n      <td>6.818</td>\n      <td>1.818</td>\n      <td>3.182</td>\n      <td>5.909</td>\n      <td>8.636</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.025</td>\n      <td>-0.029</td>\n      <td>-0.035</td>\n      <td>-0.027</td>\n      <td>-0.023</td>\n      <td>-0.020</td>\n      <td>-0.023</td>\n      <td>-0.030</td>\n      <td>-0.025</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>221</td>\n      <td>9.955</td>\n      <td>1.357</td>\n      <td>8.597</td>\n      <td>6.787</td>\n      <td>2.262</td>\n      <td>3.167</td>\n      <td>5.882</td>\n      <td>8.597</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.022</td>\n      <td>-0.026</td>\n      <td>-0.033</td>\n      <td>-0.024</td>\n      <td>-0.020</td>\n      <td>-0.017</td>\n      <td>-0.020</td>\n      <td>-0.027</td>\n      <td>-0.022</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>221</td>\n      <td>9.955</td>\n      <td>1.357</td>\n      <td>8.597</td>\n      <td>6.787</td>\n      <td>1.810</td>\n      <td>3.167</td>\n      <td>5.882</td>\n      <td>8.597</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.023</td>\n      <td>-0.027</td>\n      <td>-0.034</td>\n      <td>-0.025</td>\n      <td>-0.021</td>\n      <td>-0.018</td>\n      <td>-0.021</td>\n      <td>-0.028</td>\n      <td>-0.023</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2408</th>\n      <td>221</td>\n      <td>9.502</td>\n      <td>1.357</td>\n      <td>8.597</td>\n      <td>6.787</td>\n      <td>1.810</td>\n      <td>3.167</td>\n      <td>5.882</td>\n      <td>8.597</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.023</td>\n      <td>-0.027</td>\n      <td>-0.034</td>\n      <td>-0.025</td>\n      <td>-0.021</td>\n      <td>-0.018</td>\n      <td>-0.021</td>\n      <td>-0.028</td>\n      <td>-0.023</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2409</th>\n      <td>221</td>\n      <td>9.502</td>\n      <td>1.357</td>\n      <td>8.597</td>\n      <td>6.787</td>\n      <td>1.810</td>\n      <td>3.167</td>\n      <td>5.882</td>\n      <td>8.597</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.014</td>\n      <td>-0.018</td>\n      <td>-0.025</td>\n      <td>-0.016</td>\n      <td>-0.012</td>\n      <td>-0.009</td>\n      <td>-0.012</td>\n      <td>-0.019</td>\n      <td>-0.014</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2410</th>\n      <td>221</td>\n      <td>9.502</td>\n      <td>1.357</td>\n      <td>9.050</td>\n      <td>6.787</td>\n      <td>1.810</td>\n      <td>3.167</td>\n      <td>5.882</td>\n      <td>8.597</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.029</td>\n      <td>-0.033</td>\n      <td>-0.040</td>\n      <td>-0.031</td>\n      <td>-0.028</td>\n      <td>-0.024</td>\n      <td>-0.028</td>\n      <td>-0.035</td>\n      <td>-0.029</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2411</th>\n      <td>221</td>\n      <td>9.502</td>\n      <td>1.357</td>\n      <td>8.597</td>\n      <td>6.787</td>\n      <td>1.810</td>\n      <td>3.167</td>\n      <td>5.882</td>\n      <td>8.597</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.017</td>\n      <td>-0.021</td>\n      <td>-0.027</td>\n      <td>-0.019</td>\n      <td>-0.015</td>\n      <td>-0.012</td>\n      <td>-0.015</td>\n      <td>-0.022</td>\n      <td>-0.017</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2412</th>\n      <td>221</td>\n      <td>9.502</td>\n      <td>1.357</td>\n      <td>8.597</td>\n      <td>6.787</td>\n      <td>1.810</td>\n      <td>3.167</td>\n      <td>5.882</td>\n      <td>8.597</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.011</td>\n      <td>-0.015</td>\n      <td>-0.021</td>\n      <td>-0.013</td>\n      <td>-0.009</td>\n      <td>-0.006</td>\n      <td>-0.009</td>\n      <td>-0.016</td>\n      <td>-0.010</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n<p>2413 rows × 9298 columns</p>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c3a66853",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# scale X_test\n",
    "X_test_arr = preprocessing.MinMaxScaler().fit_transform(X_test)\n",
    "X_test_sc = pd.DataFrame(data=X_test_arr, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7b56157",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get beat_features according to previous results\n",
    "best_features = get_k_best_corrs(200, mutual_info).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df7e19a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# reduce datasets to the best features\n",
    "X_train_sc_best = X_train_sc.loc[:, best_features]\n",
    "X_test_sc_best = X_test_sc.loc[:, best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "6be7ea8c-c233-4115-8d5d-f1db724e5d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingRegressor(learning_rate=0.05, max_iter=300,\n",
       "                              max_leaf_nodes=63, min_samples_leaf=32)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit best model overall (best combination of model / method of feature selection / number of features / hyperparameters)\n",
    "estimator = HGBR\n",
    "params = best_params[3]\n",
    "model = estimator(**params)\n",
    "model.fit(X_train_sc_best, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "b6edaa75-216b-42a3-a5cb-02232635ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = pd.Series(model.predict(X_test_sc_best), name=\"tm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "2ca20fbb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get predictions and create csv file\n",
    "predictions = pd.concat([test_data[\"seq_id\"], y_preds], axis=1)\n",
    "predictions.to_csv(\"novozymes_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "\n",
    "Obtivemos um score de **0,17** (correlação de spearman entre as previsões e os valores reais de \"tm\") na competição Novozymes do KAGGLE. Uma vez que a distribuição das sequências teste é distinta da de treino (um aviso explícito do KAGGLE), era de esperar que não fossemos obter um score tão elevado quanto obtivemos na validação cruzada.\n",
    "\n",
    "É de notar ainda que não foi efeutada a divisão de dados de treino e teste, usando o dataset de treino, uma vez que era esperado que no final da competição fosse fornecido um dataset com as sequências de teste e os seus respetivos valores reais de termoestabilidade."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
